{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85cfe9fa-5a94-4795-9f6a-c50682b6317d",
      "metadata": {
        "collapsed": true,
        "id": "85cfe9fa-5a94-4795-9f6a-c50682b6317d",
        "jupyter": {
          "outputs_hidden": true
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers datasets accelerate peft bitsandbytes trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41f9c6d9-b6b0-48d1-bf37-eaf6bbb54366",
      "metadata": {
        "id": "41f9c6d9-b6b0-48d1-bf37-eaf6bbb54366",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2757e14a-4d1f-4d39-b69b-707e78d0c927",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2757e14a-4d1f-4d39-b69b-707e78d0c927",
        "outputId": "33b2a72f-de5a-46b3-e525-8e30eb32aeca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: trl\n",
            "Version: 0.17.0\n",
            "Summary: Train transformer language models with reinforcement learning.\n",
            "Home-page: https://github.com/huggingface/trl\n",
            "Author: Leandro von Werra\n",
            "Author-email: leandro.vonwerra@gmail.com\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: accelerate, datasets, rich, transformers\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6ba03340-f0a4-4c1d-a6a7-fb0fbf5f327b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ba03340-f0a4-4c1d-a6a7-fb0fbf5f327b",
        "outputId": "f1ce765e-a312-43c1-98fa-484fc04266c8",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing collected packages: shtab, tyro, trl\n",
            "  Attempting uninstall: trl\n",
            "    Found existing installation: trl 0.17.0\n",
            "    Uninstalling trl-0.17.0:\n",
            "      Successfully uninstalled trl-0.17.0\n",
            "Successfully installed shtab-1.7.2 trl-0.8.6 tyro-0.9.19\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.8)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (5.29.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install trl==0.8.6\n",
        "!pip install tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "fcbf5ef6-1ad8-4e0c-a4f0-93ef07b5985e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcbf5ef6-1ad8-4e0c-a4f0-93ef07b5985e",
        "outputId": "cf5720c5-71ed-44dc-cbfc-7b5e402fccd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files removed: 132\n"
          ]
        }
      ],
      "source": [
        "!pip cache purge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3af5187-f722-4166-9737-931f58cefd16",
      "metadata": {
        "id": "c3af5187-f722-4166-9737-931f58cefd16",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# pip install -U torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ffbaa89-6779-484b-8a96-a4f54a815207",
      "metadata": {
        "id": "6ffbaa89-6779-484b-8a96-a4f54a815207",
        "outputId": "5fb7f55f-6d54-4158-a96d-1e8571d73488",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Apr 27 19:02:32 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:1E.0 Off |                    0 |\n",
            "| N/A   40C    P0             26W /   70W |    5649MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A           38719      C   ...Detection-API/env/bin/python3        100MiB |\n",
            "|    0   N/A  N/A           38720      C   ...Detection-API/env/bin/python3       1882MiB |\n",
            "|    0   N/A  N/A           38724      C   ...Detection-API/env/bin/python3        100MiB |\n",
            "|    0   N/A  N/A           38725      C   ...Detection-API/env/bin/python3       1882MiB |\n",
            "|    0   N/A  N/A           38726      C   ...Detection-API/env/bin/python3       1682MiB |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21ad6dcd-1557-4e97-bb68-191c3d454da4",
      "metadata": {
        "id": "21ad6dcd-1557-4e97-bb68-191c3d454da4"
      },
      "outputs": [],
      "source": [
        "# !rm -rf ./gemma-finetuned-adapters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aD1S9ZsOiENT",
      "metadata": {
        "id": "aD1S9ZsOiENT"
      },
      "source": [
        "# **IMPROVED**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5161b668-f28f-468e-ba39-2de43b226523",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "cfc62f2c66924a4fa9699c935b948ccf",
            "7bbab719f10d4e4ab6e67e420dabba4b",
            "ad6742c624d14bddb6871b3d3eef2e61",
            "7cf46113a5f244e9a1709fc170389e81",
            "037aa56ec52046b3b38c28cc435ef656",
            "9dd71500aeba4562b402acfc3882c3d9",
            "e2fc75f63f0148f488ce9feec63d4777",
            "01ac06dc070d45db8de78a8649b4ad0e",
            "67f58c69ee7548c5853c80acbdefe497",
            "7355a3104c47476f8b7e6b9f42e533f5",
            "187d423fc4034cc0a10a1232b0c5c4e7",
            "09d51245ccf44088ae0f5668e9d3d9d5",
            "eaf3c23b242344ee85277bd3e9fafcee",
            "c3c9d06ea616458d8430d037d566bce9",
            "50413b245f56427e878745ea609f9439",
            "83bd570fb7934690acbebe9a862ce252",
            "704a46879a3e42629a1d36860efd7e79",
            "0da5230e1da3486bb0b8556e00499daa",
            "3a9f6313b39841c7918386895f37cde0",
            "33718572ba5e48f5aa15f2cb19911c99",
            "ee8f3d949ea64716b46b2f861113c5b8",
            "3e46f6e1448748488f21d91d8137afb3",
            "e2adc90ee82749df9b52d10584f00712",
            "51f939af465d455c988a11a5415523b0",
            "c18748d2ee0545b688dc12ef721b3ca5",
            "9b3e3bc8606e4430af5aa58562c17732",
            "93a5d7ca0d0b4bb8a8191d498ba28925",
            "2963ff3893d34c87817b1ccfb0c5afaf",
            "90a13d3617f846be889454d0908fba16",
            "b8cdbf57a4474a3184828fba840561ab",
            "5112ac84b10a4446b5903c35e41dfcee",
            "e5d4978515e44c06aeb34ac40287b2c6",
            "028af4ac533a4b9e8df1a68033188c55",
            "66620d5e2d6d42278df274790a5140ee",
            "b0b7d968c16b4060b8a4e508a02ac372",
            "75af803c409241e7b2e1ca55587e8b1d",
            "b7ee3c63932743e0817fd36ec9b8ef0d",
            "1e863cf5eb8846b7ba03975ec3a5783f",
            "59975630f23f40a3a46de2eea39f3cc7",
            "7cf7afa6bd124911b5b4283e9be24cfd",
            "6f335294b4974233b8a237ef617ec3a4",
            "12389ee735e94d938144744d4e56c06a",
            "1de63a00515d46b3bd5a8137ad002ce6",
            "f9ac8b8fac5645a0bc2df48862ea6228",
            "e297938f26704833b1d79d1f8156c778",
            "95749baf28ba4e5abb676aba27df23fa",
            "ef9f53c107a545c0b50e24b16361dab1",
            "273afce93ff84324ac5a36ae7aa53bfb",
            "55e26b0c25a84ca78b50fc728156db11",
            "db2ba0e7fa114b76815aaaabd7ec4d3c",
            "8caf937884c947208f6a92e997552056",
            "6113e27f1309421f84206cafbe358fb5",
            "d1c3b562a12940528f20e227be549142",
            "d054fc604d9c4f0fb26de1fc62fb4f73",
            "3b8d0ee4cc4b46878fdaf9bd041ab2c5",
            "fed6f52532804b73b7fd967ac541b181",
            "1b0aece87c934f1fb014f26307ba5b07",
            "261508f501d5412ea7de5a41ee90b80a",
            "04e9be96f5a746ce98fdab6511102b82",
            "90c48ab7058d427d989c1f80dcac1d36",
            "bc1195417f7142bba8b40a67d93472fa",
            "e2d7c11476cb47eb8d57d0295a4ec753",
            "133dda9cb90f4511b48b1c97e7dcdd00",
            "2d342f2ffe7c48fd99cf53bba7353692",
            "949f7cfe8ec547c38bbcb35510d705a6",
            "a08d16bde9ef4556a7382fc41a2a7a5b",
            "c5d831ca0aaf4678baf1cf4121ed81da",
            "22d8a5705a6c4b6fa97bd6160f151083",
            "1daacc7f22044f068e98acc66f25e28b",
            "58c3d53cdeb9459395233b3dd1023db3",
            "8424b39c88b04360b06b4b2e6e3b9275",
            "852fc02a5e914b4cb662b7aaf0ccfe81",
            "838a06168cf24449b180936fbfe1c232",
            "8e4e9fd5d5f14190934caf79cbe41dbd",
            "46f10eec3f4e4c62a214bace4db4508d",
            "e0df088668624a0cbb0874b3c9ace1d5",
            "cd0529ea983149bb9ee6cba5f33d8898",
            "d7f088351cc343fbaa78e7cacf8c52d0",
            "368bd9a9ade0454d93860fc5028d934d",
            "bf55a0a85c494747890f0592efe14f8c",
            "5339b7456fc843019310edffb9b8da15",
            "f2013afc5faa4ea19be50a6e3379f2a4",
            "cf15706bdd2343969d3e13c9209cd1b0",
            "0c5e22e505ef42eabdc846ccedf8584a",
            "55d1cc4c70944200a0f4fb8a52afa94f",
            "2f80ebb78f094b12ac800662ef02a49f",
            "e5f8bf04f2ad4737b96686b811479433",
            "ab401dedd62b4d92a4ad8e954e2ea11d",
            "8318bc2f20d4492f94caf34ef2b7571d",
            "511c7b16b3614cd081adf400fd6551ca",
            "faa51be94234432590e0bca4b8e3a1fb",
            "114e4083103f4cb591176b004cc36121",
            "d3abadfd0c9843a0b5aa682e6133c06b",
            "7cee91c294cb4c2686fc113c139a1bc0",
            "292af84207384707b25b29f54d860bc5",
            "549a07be71654c59ba8f6ae295088f6a",
            "41accdd5411c4a6eae76697fec3125a1",
            "a173419e200b4d158c3b4e54e9685cb5",
            "6617ddad3fd049e8a8f22615729326e5",
            "d79eaa2d5eaa4bffbe5f0d848b40e780",
            "6f48fd9c41de4eb8a7884aafd2f63748",
            "6bc32dc487504058909d4bc788d622e0",
            "e1af374ad1b0444fa295c5f1059c5d41",
            "578ef4ea74a94ec383cb0418a3dfe08d",
            "0b545b82a615456fa74b8b5b0e19d6dc",
            "52f5e9963f8642cea5bfb9acfff2e020",
            "1f828456740945bd8a785893599a91d4",
            "1dc96f8f5dbc4d94ad6c7e781a22e73f",
            "fc5c770565a44b8eb77247b9ae954ed6",
            "4499de05ebff4fb98d1722f43c7e64fd",
            "92f9f5a8debd4ae7a136815f05511acc",
            "73afcd7c0d784f439612cc45265d3d02",
            "e80d08cac52d483288c82538e5908f91",
            "d537bc58d0344ce0bac75a240ad250c3",
            "5dae7f443e6a4bf58341e40bbb92473d",
            "ebe59296438c4dd691f307d11ad5b7d4",
            "cdea6ffd2b7a493687dde71ee7595af7",
            "f00cf45ef977448e8b2ad13ccbe173fd",
            "88c4a16bc95c4c4fb8472125d520f153",
            "277d08131e074dfebf6ddf1c718cb4b0",
            "e492735f801e48d49a78853efd1bfeab",
            "d58aa9846a3240afb7ba5111a3df02a9",
            "b61208412f79400caa0d04417c6d777c",
            "faf27dcd78344eb39fda6a27ebd0b30c",
            "2d2abb9d1bae49b0a033a9f4bb3384c4",
            "cecc9dd0c22f4549be6d904fd5bd1dd0",
            "586692bc26b7478a98c17e405b21e038",
            "97126c5f837d4419a03d5d0dd802984d",
            "c7e0c8c806094c828b10aedd0b44ab46",
            "0cc85a59f30140128fc04c065e0c526c",
            "33daba5658ec4badb67146e8df10f73a",
            "b105ae15a27340c8adc6c4faf4d0ead0"
          ]
        },
        "id": "5161b668-f28f-468e-ba39-2de43b226523",
        "outputId": "91d01ec3-8515-4a23-8eb2-d887258c47a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "Device name: Tesla T4\n",
            "GPU does NOT support bfloat16. Setting bf16=False, fp16=True.\n",
            "Loading tokenizer for model: google/gemma-3-1b-it\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cfc62f2c66924a4fa9699c935b948ccf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09d51245ccf44088ae0f5668e9d3d9d5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2adc90ee82749df9b52d10584f00712",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "66620d5e2d6d42278df274790a5140ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e297938f26704833b1d79d1f8156c778",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading base model: google/gemma-3-1b-it with QLoRA config\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fed6f52532804b73b7fd967ac541b181",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/899 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5d831ca0aaf4678baf1cf4121ed81da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7f088351cc343fbaa78e7cacf8c52d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset from: llm_training_data_claude-22.jsonl\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8318bc2f20d4492f94caf34ef2b7571d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded successfully. Number of examples: 596\n",
            "Formatting dataset...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d79eaa2d5eaa4bffbe5f0d848b40e780",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/596 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "92f9f5a8debd4ae7a136815f05511acc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/596 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset formatted and filtered.\n",
            "Example formatted text:\n",
            "<s>[INST] Extract the following information from the given text of driving license: Address, First_Name, Height, Last_Name, Sex, State\n",
            "\n",
            "Text: Driver\n",
            "ucend - No\n",
            "j U 216 *\n",
            "Licence\n",
            "JONES MARGARET CHERYL\n",
            "50 SMITH ST\n",
            "SMITHVILLE QLD 9999\n",
            "itninre\n",
            "aply\n",
            "26-03-80 F 170\n",
            "01-07-2006 30-06-2011\n",
            "Queensland\n",
            "Goverament\n",
            "eens and Transport [/INST] {\"Last_Name\": \"JONES\", \"First_Name\": \"MARGARET CHERYL\", \"Address\": \"50 SMITH ST\\nSMITHVILLE QLD 9999\", \"Sex\": \"F\", \"Height\": \"170\", \"State\": \"Queensland\"} </s>\n",
            "Initializing SFTTrainer...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d58aa9846a3240afb7ba5111a3df02a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/596 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:323: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='55' max='37000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   55/37000 01:52 < 21:45:53, 0.47 it/s, Epoch 0.72/500]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>2.272800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.664300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-265fb4dc7c16>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;31m# If you had a previous checkpoint and want to resume, uncomment the line below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;31m# trainer.train(resume_from_checkpoint=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Start training from scratch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training finished.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trl_activate_neftune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;31m# After training we make sure to retrieve back the original forward pass method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2558\u001b[0m                     )\n\u001b[1;32m   2559\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2560\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2562\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3780\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3782\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3784\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2448\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2449\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2450\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2451\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_lomo_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2452\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "# from huggingface_hub import notebook_login # Uncomment if using notebook login\n",
        "\n",
        "# --- Configuration ---\n",
        "MODEL_ID = \"google/gemma-3-1b-it\"\n",
        "DATASET_PATH = \"llm_training_data_claude-22.jsonl\" # Replace with your dataset path\n",
        "OUTPUT_DIR = \"./gemma-finetuned-adapters\" # Will be created fresh\n",
        "\n",
        "# --- Authentication ---\n",
        "# IMPORTANT: Replace the placeholder token below with your actual Hugging Face token.\n",
        "# Using a manual token string like this is ONE way to authenticate.\n",
        "# The recommended way is usually `huggingface-cli login` or `notebook_login()`.\n",
        "# Choose ONE method only. If you set the token here, you don't need to run login commands.\n",
        "token = \"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\" # Replace with your actual token from HF settings\n",
        "\n",
        "# Make sure you are logged in via `huggingface-cli login` OR uncomment and run:\n",
        "# notebook_login() # Only if you are in a notebook and token is None\n",
        "\n",
        "# --- QLoRA Configuration ---\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16, # Set based on GPU check below\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "# --- LoRA Configuration ---\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    target_modules=[ \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\" ],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# --- Training Arguments ---\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=500,             # Starting with 3 epochs (adjust as needed)\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4, # Effective batch size = per_device_train_batch_size * gradient_accumulation_steps\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,                 # Save checkpoint every 100 steps (adjust as needed)\n",
        "    logging_steps=25,               # Log every 25 steps (adjust as needed)\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    fp16=False,                     # Will be set based on GPU check\n",
        "    bf16=False,                     # Will be set based on GPU check\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,                   # Train for num_train_epochs, or set a positive value for max steps\n",
        "    warmup_ratio=0.03,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "# --- Check GPU Availability & Set Precisions ---\n",
        "if not torch.cuda.is_available():\n",
        "    raise SystemError(\"CUDA is not available.\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
        "if torch.cuda.get_device_capability(0)[0] >= 8:\n",
        "    print(\"GPU supports bfloat16. Setting bf16=True, fp16=False.\")\n",
        "    training_arguments.bf16 = True\n",
        "    training_arguments.fp16 = False\n",
        "    bnb_config.bnb_4bit_compute_dtype = torch.bfloat16\n",
        "else:\n",
        "    print(\"GPU does NOT support bfloat16. Setting bf16=False, fp16=True.\")\n",
        "    training_arguments.bf16 = False\n",
        "    training_arguments.fp16 = True\n",
        "    bnb_config.bnb_4bit_compute_dtype = torch.float16\n",
        "\n",
        "\n",
        "# --- Load Tokenizer and Model ---\n",
        "print(f\"Loading tokenizer for model: {MODEL_ID}\")\n",
        "# This dictionary correctly adds the 'token' argument if the token variable is set\n",
        "tokenizer_kwargs = {\"trust_remote_code\": True}\n",
        "if token: tokenizer_kwargs[\"token\"] = token\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, **tokenizer_kwargs)\n",
        "if tokenizer.pad_token is None:\n",
        "    print(\"Setting pad_token = eos_token\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(f\"Loading base model: {MODEL_ID} with QLoRA config\")\n",
        "# This dictionary correctly adds the 'token' argument if the token variable is set\n",
        "model_kwargs = {\n",
        "    \"quantization_config\": bnb_config,\n",
        "    \"device_map\": \"auto\",\n",
        "    \"trust_remote_code\": True,\n",
        "    \"attn_implementation\": \"eager\", # Use eager attention for Gemma3 (Recommended by HF docs)\n",
        "}\n",
        "if token: model_kwargs[\"token\"] = token\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, **model_kwargs)\n",
        "\n",
        "# --- Load Dataset ---\n",
        "print(f\"Loading dataset from: {DATASET_PATH}\")\n",
        "try:\n",
        "    # Assuming the dataset is a JSON Lines file\n",
        "    dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
        "    print(f\"Dataset loaded successfully. Number of examples: {len(dataset)}\")\n",
        "except FileNotFoundError:\n",
        "     print(f\"Error: Dataset file not found at {DATASET_PATH}\"); exit()\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\"); exit()\n",
        "\n",
        "# --- Preprocess Dataset ---\n",
        "# This function formats each data sample into a single text string\n",
        "# suitable for Causal LM training, using the Gemma-specific instruction format.\n",
        "def format_instruction(sample):\n",
        "    # Extract instruction and output, handle potential None or empty strings\n",
        "    instruction = sample.get('instruction', '') or ''\n",
        "    output = sample.get('output', '') or ''\n",
        "\n",
        "    # Clean up potential leftover tokens/whitespace from data source\n",
        "    instruction = instruction.strip().removeprefix(\"<s>\").removesuffix(\"</s>\").strip()\n",
        "    output = output.strip().removeprefix(\"<s>\").removesuffix(\"</s>\").strip()\n",
        "\n",
        "    # Format according to Gemma's instruction format: <s>[INST] Instruction [/INST] Output </s>\n",
        "    # Ensure there's content before formatting\n",
        "    if not instruction:\n",
        "        print(f\"Warning: Skipping sample with no instruction: {sample}\")\n",
        "        return {\"text\": \"\"} # Return empty text for samples to be filtered out\n",
        "    if not output:\n",
        "         print(f\"Warning: Sample with instruction but no output: {sample}\")\n",
        "         # Optionally format without output if you want to train on prompts only\n",
        "         # For this setup (SFT), we usually expect prompt-response pairs\n",
        "         # Returning empty text will effectively skip this sample for training\n",
        "         return {\"text\": \"\"}\n",
        "\n",
        "\n",
        "    return f\"<s>[INST] {instruction} [/INST] {output} </s>\"\n",
        "\n",
        "print(\"Formatting dataset...\")\n",
        "try:\n",
        "    # Apply the formatting function to each sample\n",
        "    dataset = dataset.map(lambda sample: {\"text\": format_instruction(sample)})\n",
        "    # Filter out samples that resulted in empty text (e.g., missing instruction/output)\n",
        "    dataset = dataset.filter(lambda sample: sample[\"text\"] != \"\")\n",
        "\n",
        "    print(\"Dataset formatted and filtered.\")\n",
        "    if not dataset: print(\"Warning: Processed dataset is empty. Check your data file and formatting function.\"); exit()\n",
        "    print(f\"Example formatted text:\\n{dataset[0]['text']}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error formatting dataset: {e}\"); exit()\n",
        "\n",
        "# --- Initialize Trainer ---\n",
        "print(\"Initializing SFTTrainer...\")\n",
        "# SFTTrainer handles wrapping the base model with LoRA adapters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=lora_config,\n",
        "    dataset_text_field=\"text\", # The column containing the formatted text\n",
        "    max_seq_length=512,        # Maximum sequence length for training\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=False,             # Set to True for more efficient training if max_seq_length varies widely\n",
        ")\n",
        "\n",
        "# --- Train the Model ---\n",
        "print(\"Starting training...\")\n",
        "try:\n",
        "    # If you had a previous checkpoint and want to resume, uncomment the line below\n",
        "    # trainer.train(resume_from_checkpoint=True)\n",
        "    trainer.train() # Start training from scratch\n",
        "    print(\"Training finished.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during training: {e}\"); exit()\n",
        "\n",
        "# --- Save the Trained Adapters ---\n",
        "# SFTTrainer automatically saves the model/adapters to output_dir\n",
        "print(f\"Saving LoRA adapters to: {OUTPUT_DIR}\")\n",
        "trainer.save_model()\n",
        "print(\"Adapters saved successfully.\")\n",
        "\n",
        "# --- Optional: Clean up memory ---\n",
        "# Free up GPU memory by deleting model and trainer objects\n",
        "print(\"Cleaning up memory...\")\n",
        "if 'model' in locals(): del model\n",
        "if 'trainer' in locals(): del trainer\n",
        "# Additional manual cleanup for objects potentially holding onto memory\n",
        "if 'tokenizer' in locals() and hasattr(tokenizer, 'model'): del tokenizer.model\n",
        "if 'pipe' in locals() and hasattr(pipe, 'model'): del pipe.model\n",
        "if 'inf_model' in locals(): del inf_model\n",
        "torch.cuda.empty_cache() # Clear cached memory\n",
        "print(\"Memory cleaned.\")\n",
        "\n",
        "# --- Optional: Test Inference ---\n",
        "# Load the base model again and merge the adapters for inference\n",
        "print(\"\\n--- Testing Inference with Adapters ---\")\n",
        "# Set logging verbosity to critical to avoid excessive output from transformers\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "try:\n",
        "    # Load base model for inference - use float16 compute dtype typically for inference\n",
        "    print(\"Loading base model for inference...\")\n",
        "    # Use a fresh BitsAndBytesConfig for inference if needed, or reuse the training one\n",
        "    # Ensure compute_dtype is compatible with your GPU for inference, often float16 is safe\n",
        "    inf_bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16, # Use float16 for inference compute\n",
        "        bnb_4bit_use_double_quant=False )\n",
        "    inf_model_kwargs = { \"quantization_config\": inf_bnb_config, \"device_map\": \"auto\",\n",
        "                         \"trust_remote_code\": True, \"attn_implementation\": \"eager\" }\n",
        "    if token: inf_model_kwargs[\"token\"] = token # Pass token for inference model load too\n",
        "    inf_model = AutoModelForCausalLM.from_pretrained(MODEL_ID, **inf_model_kwargs)\n",
        "\n",
        "    # Load the trained LoRA adapters\n",
        "    print(f\"Loading adapters from: {OUTPUT_DIR}\")\n",
        "    # PeftModel wraps the base model and applies the adapters\n",
        "    inf_model = PeftModel.from_pretrained(inf_model, OUTPUT_DIR)\n",
        "    print(\"Adapters loaded onto base model.\")\n",
        "\n",
        "    # Load the tokenizer again for inference\n",
        "    inf_tokenizer_kwargs = {\"trust_remote_code\": True}\n",
        "    if token: inf_tokenizer_kwargs[\"token\"] = token # Pass token for inference tokenizer load\n",
        "    inf_tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, **inf_tokenizer_kwargs)\n",
        "    if inf_tokenizer.pad_token is None: inf_tokenizer.pad_token = inf_tokenizer.eos_token\n",
        "    inf_tokenizer.padding_side = \"right\" # For generation, padding side should be 'right'\n",
        "\n",
        "    # Prepare a test prompt from the dataset's first example\n",
        "    # We need to extract just the instruction part for the prompt\n",
        "    test_formatted_text = dataset[0]['text']\n",
        "    inst_end_marker = \"[/INST]\"\n",
        "    prompt_end_index = test_formatted_text.find(inst_end_marker)\n",
        "\n",
        "    if prompt_end_index != -1:\n",
        "        # Extract only the part up to and including the instruction end marker\n",
        "        # This simulates giving the model an instruction to complete\n",
        "        prompt_for_model = test_formatted_text[:prompt_end_index + len(inst_end_marker)].strip()\n",
        "    else:\n",
        "        # Fallback in case the formatting wasn't perfect, try to reconstruct the prompt\n",
        "        # This might not be ideal if the formatting function changes\n",
        "        print(\"Warning: Could not find [/INST] marker in formatted text. Attempting to reconstruct prompt.\")\n",
        "        prompt_for_model = f\"<s>[INST] {dataset[0].get('instruction', '').strip()} [/INST]\"\n",
        "\n",
        "    # Ensure the prompt starts with the correct BOS token if not already present\n",
        "    if not prompt_for_model.startswith(inf_tokenizer.bos_token):\n",
        "         prompt_for_model = inf_tokenizer.bos_token + prompt_for_model\n",
        "\n",
        "    print(f\"\\nTest Prompt:\\n---\\n{prompt_for_model}\\n---\")\n",
        "\n",
        "    # Create a text generation pipeline using the adapted model and tokenizer\n",
        "    pipe = pipeline(task=\"text-generation\", model=inf_model, tokenizer=inf_tokenizer, max_new_tokens=150)\n",
        "\n",
        "    # Generate text based on the prompt\n",
        "    # The pipe will handle tokenizing the prompt, running inference, and decoding\n",
        "    result = pipe(prompt_for_model)\n",
        "\n",
        "    print(\"\\nGenerated Output:\")\n",
        "    # The result is a list of dictionaries. We take the text from the first result.\n",
        "    generated_text = result[0]['generated_text']\n",
        "\n",
        "    # Post-process the generated text to show only the generated response\n",
        "    # Find the end of the prompt in the generated text\n",
        "    prompt_end_in_generated = generated_text.find(prompt_for_model)\n",
        "    if prompt_end_in_generated != -1:\n",
        "      # Start of the generated response is after the prompt\n",
        "      answer_start_index = prompt_end_in_generated + len(prompt_for_model)\n",
        "      answer = generated_text[answer_start_index:].strip()\n",
        "\n",
        "      # Remove the final EOS token if it exists\n",
        "      if answer.endswith(inf_tokenizer.eos_token):\n",
        "          answer = answer[:-len(inf_tokenizer.eos_token)].strip()\n",
        "      print(answer)\n",
        "    else:\n",
        "      # If the prompt wasn't found in the generated text (shouldn't happen with pipeline),\n",
        "      # print the whole generation or a warning\n",
        "      print(\"Warning: Prompt not found in generated text. Printing full output:\")\n",
        "      print(generated_text)\n",
        "\n",
        "\n",
        "    # Print the expected output from the dataset for comparison\n",
        "    print(\"\\nExpected Output (from dataset):\")\n",
        "    print(dataset[0].get('output', 'N/A')) # Use .get for safety\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nError during inference testing: {e}\")\n",
        "\n",
        "print(\"\\n--- Script Finished ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "E8zVSM2ZiXNT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5d9143426760482e932651002cdd665d",
            "8190b6d1997f4e1c89773265f0392fba",
            "4c10446103644f0bac03cc66dc30a475",
            "9ff35b4a29f241fd8bbf1c166b0b5ca9",
            "0e1c743b772e42aa9766eb7c34e92bec",
            "afb6fa57faf24bdd9341adbb072a7882",
            "4e0bad8e4cea465d9b7edac93e7d0456",
            "c99c03f2f0494ffa8e6593aaf66f4bbe",
            "339ae78fe88e4931b48149b466bf7a97",
            "9775dcfdc68142d58650b7c7ff3a9d7d",
            "9b05c3aabc094d5899a99601c901b57d"
          ]
        },
        "id": "E8zVSM2ZiXNT",
        "outputId": "cc39ff52-aa52-4d05-d152-b8ac87d96172"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "Device name: Tesla T4\n",
            "GPU does NOT support bfloat16. Setting bf16=False, fp16=True.\n",
            "Loading tokenizer for model: google/gemma-3-4b-it\n",
            "Loading base model: google/gemma-3-4b-it with QLoRA config\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d9143426760482e932651002cdd665d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset from: llm_training_data_claude-22.jsonl\n",
            "Dataset loaded successfully. Number of examples: 596\n",
            "Formatting dataset...\n",
            "Dataset formatted and filtered.\n",
            "Example formatted text:\n",
            "<s>[INST] Extract the following information from the given text of driving license: Address, First_Name, Height, Last_Name, Sex, State\n",
            "\n",
            "Text: Driver\n",
            "ucend - No\n",
            "j U 216 *\n",
            "Licence\n",
            "JONES MARGARET CHERYL\n",
            "50 SMITH ST\n",
            "SMITHVILLE QLD 9999\n",
            "itninre\n",
            "aply\n",
            "26-03-80 F 170\n",
            "01-07-2006 30-06-2011\n",
            "Queensland\n",
            "Goverament\n",
            "eens and Transport [/INST] {\"Last_Name\": \"JONES\", \"First_Name\": \"MARGARET CHERYL\", \"Address\": \"50 SMITH ST\\nSMITHVILLE QLD 9999\", \"Sex\": \"F\", \"Height\": \"170\", \"State\": \"Queensland\"} </s>\n",
            "Initializing SFTTrainer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:323: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3147' max='74500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 3147/74500 3:16:27 < 74:17:08, 0.27 it/s, Epoch 21.11/500]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>9.315500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>5.673200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>5.429200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>4.924600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>4.955400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>4.630400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>4.014700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>3.763900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>3.922200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>4.180800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>4.016700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>3.810900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>3.030600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>3.028900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>375</td>\n",
              "      <td>3.404800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>3.146400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>425</td>\n",
              "      <td>3.141100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>3.168400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>475</td>\n",
              "      <td>2.467700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.422900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>525</td>\n",
              "      <td>2.646300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>2.392800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>575</td>\n",
              "      <td>2.339600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>2.378900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>625</td>\n",
              "      <td>1.734600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.793700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>675</td>\n",
              "      <td>1.857500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.857800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>725</td>\n",
              "      <td>2.144700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.775100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>775</td>\n",
              "      <td>1.244500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.489000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>825</td>\n",
              "      <td>1.321900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>1.380500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>875</td>\n",
              "      <td>1.565700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.374200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>925</td>\n",
              "      <td>0.910200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>1.090000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>975</td>\n",
              "      <td>0.961000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.077900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1025</td>\n",
              "      <td>1.213300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>1.054100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1075</td>\n",
              "      <td>0.698800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.875600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1125</td>\n",
              "      <td>0.862500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.866700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1175</td>\n",
              "      <td>0.921400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.772100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1225</td>\n",
              "      <td>0.595200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.732400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1275</td>\n",
              "      <td>0.715700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.728200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1325</td>\n",
              "      <td>0.817400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>0.701000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1375</td>\n",
              "      <td>0.535300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.593600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1425</td>\n",
              "      <td>0.600500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>0.620600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1475</td>\n",
              "      <td>0.717400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.557100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1525</td>\n",
              "      <td>0.471600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>0.571700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1575</td>\n",
              "      <td>0.576000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.537700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1625</td>\n",
              "      <td>0.598000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>0.517700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1675</td>\n",
              "      <td>0.426300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.521400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1725</td>\n",
              "      <td>0.490600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>0.496100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1775</td>\n",
              "      <td>0.590700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.441100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1825</td>\n",
              "      <td>0.372700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>0.437100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1875</td>\n",
              "      <td>0.476400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.471300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1925</td>\n",
              "      <td>0.496000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>0.433400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1975</td>\n",
              "      <td>0.377700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.440300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2025</td>\n",
              "      <td>0.449300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2050</td>\n",
              "      <td>0.421000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2075</td>\n",
              "      <td>0.463700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.397000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2125</td>\n",
              "      <td>0.331100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2150</td>\n",
              "      <td>0.389200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2175</td>\n",
              "      <td>0.393900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.417400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2225</td>\n",
              "      <td>0.424600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>0.365800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2275</td>\n",
              "      <td>0.351100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.369800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2325</td>\n",
              "      <td>0.380000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2350</td>\n",
              "      <td>0.400400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2375</td>\n",
              "      <td>0.398300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.344600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2425</td>\n",
              "      <td>0.318500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2450</td>\n",
              "      <td>0.359800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2475</td>\n",
              "      <td>0.356700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.385500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2525</td>\n",
              "      <td>0.368300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2550</td>\n",
              "      <td>0.321700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2575</td>\n",
              "      <td>0.300600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.346000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2625</td>\n",
              "      <td>0.355600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2650</td>\n",
              "      <td>0.363200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2675</td>\n",
              "      <td>0.335500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.312100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2725</td>\n",
              "      <td>0.302100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2750</td>\n",
              "      <td>0.322500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2775</td>\n",
              "      <td>0.339600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.330600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2825</td>\n",
              "      <td>0.353400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2850</td>\n",
              "      <td>0.310600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2875</td>\n",
              "      <td>0.291300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.317500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2925</td>\n",
              "      <td>0.308700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2950</td>\n",
              "      <td>0.309900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2975</td>\n",
              "      <td>0.324100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.270900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3025</td>\n",
              "      <td>0.273500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3050</td>\n",
              "      <td>0.293300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3075</td>\n",
              "      <td>0.318300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>0.310700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3125</td>\n",
              "      <td>0.310500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f03bf-6d4912e04321cbd53e700df1;06e9b5a9-65f5-4e9b-9430-d64679ad4ab6)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-3-4b-it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-3-4b-it - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f0535-493c2be00fcbdce80fa7e185;77713a34-cf74-452e-98e2-94825e46c237)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-3-4b-it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-3-4b-it - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f06a6-5e9d7aeb4a629ffc4017abb4;204db230-804d-415d-a43c-800a886a5f6d)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-3-4b-it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-3-4b-it - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f081f-6aa4e3cb4e7755d771b0ec03;57ba1fd9-a7a3-4a73-b05c-7bb3edd40163)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-3-4b-it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-3-4b-it - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f0993-0685994a67bf16247caf344d;7aa8e943-9439-4174-90ca-0c1cd2cca33a)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-3-4b-it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-3-4b-it - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f0b05-17c48806373ccf393513a2d4;ffda490b-570a-4ffc-8c36-49a663bcda7e)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-3-4b-it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-3-4b-it - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f0c80-4ad31a5a65a1f2f17f8dd67e;baea26ca-0d21-43eb-b4dd-8f56f5a1b89f)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-3-4b-it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-3-4b-it - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f0dfb-77a3130c0548f041566107cc;cbf0f56a-0ab1-4a73-aa4f-d38551052be9)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-3-4b-it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-3-4b-it - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f0f71-034114d50c6798a371a495d1;60ac93b9-fd74-4858-9eed-6248f2bc0a60)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-3-4b-it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-3-4b-it - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f10ea-09b304f82cec9bf745b9811f;4446cd39-f1cc-4476-8028-eef32f5c3b64)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-3-4b-it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-3-4b-it - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f1264-7c5cf26d7904186311615adf;a4f89420-67f9-4363-8414-fca4382b7c87)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-3-4b-it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-3-4b-it - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f13d7-739411691bb1c7c1144753a7;fa74c86d-952f-47d0-a8db-9dffa3776663)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-3-4b-it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-3-4b-it - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f154b-154611582e2ade914bf59773;5a1f96ea-0a0d-441d-921f-f5a911470fbc)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-3-4b-it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-3-4b-it - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f16c1-450ad7657de46ed056cab16f;a175a095-2e86-448c-b8c8-515071d71897)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-3-4b-it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-3-4b-it - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f1834-0ae0c80622a06e2a6e3aac09;8dc0ecfd-06cf-4bf4-9e6d-ed0b971d0119)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-3-4b-it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-3-4b-it - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f19a2-5c1464334a32f79f576a9633;68b22cf2-80c5-4c91-a60e-3b04b043856e)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-3-4b-it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-3-4b-it - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f1b1d-41504ba200f7e8390222d87a;fa451825-d150-40cf-986f-59820ca30476)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-3-4b-it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-3-4b-it - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f1c91-1869d8751d660fda0611c80e;618d9b5e-502b-4e9f-9bce-4c27c28bcbdb)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-3-4b-it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-3-4b-it - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f1e00-6ee32d1211d249485ddcf15d;ecf6389d-fd4f-4764-8e4a-b8f5191e3718)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-3-4b-it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-3-4b-it - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f1f79-343392d15a79616c4b090df3;94d2c168-e015-44f5-9e71-6019eb47956a)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-3-4b-it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-3-4b-it - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f20f3-655cc7ac4334f3ce6575d0e5;46f9021c-257f-44b6-8e59-c92a88b09ecb)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-3-4b-it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-3-4b-it - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f2268-309c630f4acdf4000f15174c;7b4fdb85-8c34-4547-aa0e-39e48ca88500)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-3-4b-it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-3-4b-it - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f23e0-1fe8e6472d86167945540c68;65bc723c-5ac3-4cbc-84f0-e1340ef14820)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-3-4b-it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-3-4b-it - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f2557-159e56907e94b13b0d84bd46;f09ecd60-2d23-4b67-a30d-c3e6c1141e9e)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-3-4b-it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-3-4b-it - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f26cc-78e7d4420c1c0b1159b5235b;04036eba-e64b-4e2d-a2ee-9f17d59d0859)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-3-4b-it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-3-4b-it - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f2843-560d2ead7ab857196afcf561;0944ec25-5e04-4184-aba2-e45b2d223613)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-3-4b-it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-3-4b-it - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f29be-5c2a049a475f0fb64b796d59;17d05e58-8cf9-43f4-a63f-5410f2078026)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-3-4b-it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-3-4b-it - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f2b34-5f03a1e655b7ba97203c7e27;aab36a1d-f023-49ef-b6a3-5102aae92029)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-3-4b-it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-3-4b-it - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f2cad-4ed5ec913aa0b4bf1b562888;090331b6-24b6-4335-97a3-acdf3fc5e39f)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-3-4b-it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-3-4b-it - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f2e28-6e11042c7214d17f45ca020c;4a880cc8-3a90-4864-9a9c-04992c768ad5)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-3-4b-it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-3-4b-it - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f2fa3-6b2c52ce52dd153e629fcbdb;bdfb2350-6fa3-4b8c-9d37-1b506f488ec7)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-3-4b-it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-3-4b-it - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7c000f0d526f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;31m# If you had a previous checkpoint and want to resume, uncomment the line below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;31m# trainer.train(resume_from_checkpoint=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Start training from scratch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training finished.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trl_activate_neftune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;31m# After training we make sure to retrieve back the original forward pass method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2558\u001b[0m                     )\n\u001b[1;32m   2559\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2560\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2562\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3735\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3736\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3799\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3800\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3801\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3802\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1755\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1756\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   1758\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1759\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma3/modeling_gemma3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, token_type_ids, cache_position, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, logits_to_keep, **lm_kwargs)\u001b[0m\n\u001b[1;32m   1324\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_position\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0;31m         outputs: CausalLMOutputWithPast = self.language_model(\n\u001b[0m\u001b[1;32m   1327\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma3/modeling_gemma3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m    940\u001b[0m         )\n\u001b[1;32m    941\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    943\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma3/modeling_gemma3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, last_cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 )\n\u001b[1;32m    721\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    723\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m                     \u001b[0mposition_embeddings_global\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_embeddings_global\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma3/modeling_gemma3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings_global, position_embeddings_local, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, last_cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposition_embeddings_global\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         hidden_states, self_attn_weights = self.self_attn(\n\u001b[0m\u001b[1;32m    421\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma3/modeling_gemma3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mhidden_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mquery_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    500\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mactive_adapter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_adapters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mactive_adapter\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlora_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m                         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mactive_adapters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_adapter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;31m# is already a list of str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_adapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0menable_adapters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menabled\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mactive_adapter\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_disable_adapters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mactive_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;31m# use a property to ensure that active_adapter is not set directly, instead use the set_adapter method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "# from huggingface_hub import notebook_login # Uncomment if using notebook login\n",
        "\n",
        "# --- Configuration ---\n",
        "MODEL_ID = \"google/gemma-3-4b-it\"\n",
        "DATASET_PATH = \"llm_training_data_claude-22.jsonl\" # Replace with your dataset path\n",
        "OUTPUT_DIR = \"./gemma-finetuned-adapters\" # Will be created fresh\n",
        "\n",
        "# --- Authentication ---\n",
        "# IMPORTANT: Replace the placeholder token below with your actual Hugging Face token.\n",
        "# Using a manual token string like this is ONE way to authenticate.\n",
        "# The recommended way is usually `huggingface-cli login` or `notebook_login()`.\n",
        "# Choose ONE method only. If you set the token here, you don't need to run login commands.\n",
        "token = \"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\" # Replace with your actual token from HF settings\n",
        "\n",
        "# Make sure you are logged in via `huggingface-cli login` OR uncomment and run:\n",
        "# notebook_login() # Only if you are in a notebook and token is None\n",
        "\n",
        "# --- QLoRA Configuration ---\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16, # Set based on GPU check below\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "# --- LoRA Configuration ---\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    target_modules=[ \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\" ],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# --- Training Arguments ---\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=500,             # Starting with 3 epochs (adjust as needed)\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4, # Effective batch size = per_device_train_batch_size * gradient_accumulation_steps\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,                 # Save checkpoint every 100 steps (adjust as needed)\n",
        "    logging_steps=25,               # Log every 25 steps (adjust as needed)\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    fp16=False,                     # Will be set based on GPU check\n",
        "    bf16=False,                     # Will be set based on GPU check\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,                   # Train for num_train_epochs, or set a positive value for max steps\n",
        "    warmup_ratio=0.03,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "# --- Check GPU Availability & Set Precisions ---\n",
        "if not torch.cuda.is_available():\n",
        "    raise SystemError(\"CUDA is not available.\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
        "if torch.cuda.get_device_capability(0)[0] >= 8:\n",
        "    print(\"GPU supports bfloat16. Setting bf16=True, fp16=False.\")\n",
        "    training_arguments.bf16 = True\n",
        "    training_arguments.fp16 = False\n",
        "    bnb_config.bnb_4bit_compute_dtype = torch.bfloat16\n",
        "else:\n",
        "    print(\"GPU does NOT support bfloat16. Setting bf16=False, fp16=True.\")\n",
        "    training_arguments.bf16 = False\n",
        "    training_arguments.fp16 = True\n",
        "    bnb_config.bnb_4bit_compute_dtype = torch.float16\n",
        "\n",
        "\n",
        "# --- Load Tokenizer and Model ---\n",
        "print(f\"Loading tokenizer for model: {MODEL_ID}\")\n",
        "# This dictionary correctly adds the 'token' argument if the token variable is set\n",
        "tokenizer_kwargs = {\"trust_remote_code\": True}\n",
        "if token: tokenizer_kwargs[\"token\"] = token\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, **tokenizer_kwargs)\n",
        "if tokenizer.pad_token is None:\n",
        "    print(\"Setting pad_token = eos_token\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(f\"Loading base model: {MODEL_ID} with QLoRA config\")\n",
        "# This dictionary correctly adds the 'token' argument if the token variable is set\n",
        "model_kwargs = {\n",
        "    \"quantization_config\": bnb_config,\n",
        "    \"device_map\": \"auto\",\n",
        "    \"trust_remote_code\": True,\n",
        "    \"attn_implementation\": \"eager\", # Use eager attention for Gemma3 (Recommended by HF docs)\n",
        "}\n",
        "if token: model_kwargs[\"token\"] = token\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, **model_kwargs)\n",
        "\n",
        "# --- Load Dataset ---\n",
        "print(f\"Loading dataset from: {DATASET_PATH}\")\n",
        "try:\n",
        "    # Assuming the dataset is a JSON Lines file\n",
        "    dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
        "    print(f\"Dataset loaded successfully. Number of examples: {len(dataset)}\")\n",
        "except FileNotFoundError:\n",
        "     print(f\"Error: Dataset file not found at {DATASET_PATH}\"); exit()\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\"); exit()\n",
        "\n",
        "# --- Preprocess Dataset ---\n",
        "# This function formats each data sample into a single text string\n",
        "# suitable for Causal LM training, using the Gemma-specific instruction format.\n",
        "def format_instruction(sample):\n",
        "    # Extract instruction and output, handle potential None or empty strings\n",
        "    instruction = sample.get('instruction', '') or ''\n",
        "    output = sample.get('output', '') or ''\n",
        "\n",
        "    # Clean up potential leftover tokens/whitespace from data source\n",
        "    instruction = instruction.strip().removeprefix(\"<s>\").removesuffix(\"</s>\").strip()\n",
        "    output = output.strip().removeprefix(\"<s>\").removesuffix(\"</s>\").strip()\n",
        "\n",
        "    # Format according to Gemma's instruction format: <s>[INST] Instruction [/INST] Output </s>\n",
        "    # Ensure there's content before formatting\n",
        "    if not instruction:\n",
        "        print(f\"Warning: Skipping sample with no instruction: {sample}\")\n",
        "        return {\"text\": \"\"} # Return empty text for samples to be filtered out\n",
        "    if not output:\n",
        "         print(f\"Warning: Sample with instruction but no output: {sample}\")\n",
        "         # Optionally format without output if you want to train on prompts only\n",
        "         # For this setup (SFT), we usually expect prompt-response pairs\n",
        "         # Returning empty text will effectively skip this sample for training\n",
        "         return {\"text\": \"\"}\n",
        "\n",
        "\n",
        "    return f\"<s>[INST] {instruction} [/INST] {output} </s>\"\n",
        "\n",
        "print(\"Formatting dataset...\")\n",
        "try:\n",
        "    # Apply the formatting function to each sample\n",
        "    dataset = dataset.map(lambda sample: {\"text\": format_instruction(sample)})\n",
        "    # Filter out samples that resulted in empty text (e.g., missing instruction/output)\n",
        "    dataset = dataset.filter(lambda sample: sample[\"text\"] != \"\")\n",
        "\n",
        "    print(\"Dataset formatted and filtered.\")\n",
        "    if not dataset: print(\"Warning: Processed dataset is empty. Check your data file and formatting function.\"); exit()\n",
        "    print(f\"Example formatted text:\\n{dataset[0]['text']}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error formatting dataset: {e}\"); exit()\n",
        "\n",
        "# --- Initialize Trainer ---\n",
        "print(\"Initializing SFTTrainer...\")\n",
        "# SFTTrainer handles wrapping the base model with LoRA adapters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=lora_config,\n",
        "    dataset_text_field=\"text\", # The column containing the formatted text\n",
        "    max_seq_length=512,        # Maximum sequence length for training\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=False,             # Set to True for more efficient training if max_seq_length varies widely\n",
        ")\n",
        "\n",
        "# --- Train the Model ---\n",
        "print(\"Starting training...\")\n",
        "try:\n",
        "    # If you had a previous checkpoint and want to resume, uncomment the line below\n",
        "    # trainer.train(resume_from_checkpoint=True)\n",
        "    trainer.train() # Start training from scratch\n",
        "    print(\"Training finished.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during training: {e}\"); exit()\n",
        "\n",
        "# --- Save the Trained Adapters ---\n",
        "# SFTTrainer automatically saves the model/adapters to output_dir\n",
        "print(f\"Saving LoRA adapters to: {OUTPUT_DIR}\")\n",
        "trainer.save_model()\n",
        "print(\"Adapters saved successfully.\")\n",
        "\n",
        "# --- Optional: Clean up memory ---\n",
        "# Free up GPU memory by deleting model and trainer objects\n",
        "print(\"Cleaning up memory...\")\n",
        "if 'model' in locals(): del model\n",
        "if 'trainer' in locals(): del trainer\n",
        "# Additional manual cleanup for objects potentially holding onto memory\n",
        "if 'tokenizer' in locals() and hasattr(tokenizer, 'model'): del tokenizer.model\n",
        "if 'pipe' in locals() and hasattr(pipe, 'model'): del pipe.model\n",
        "if 'inf_model' in locals(): del inf_model\n",
        "torch.cuda.empty_cache() # Clear cached memory\n",
        "print(\"Memory cleaned.\")\n",
        "\n",
        "# --- Optional: Test Inference ---\n",
        "# Load the base model again and merge the adapters for inference\n",
        "print(\"\\n--- Testing Inference with Adapters ---\")\n",
        "# Set logging verbosity to critical to avoid excessive output from transformers\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "try:\n",
        "    # Load base model for inference - use float16 compute dtype typically for inference\n",
        "    print(\"Loading base model for inference...\")\n",
        "    # Use a fresh BitsAndBytesConfig for inference if needed, or reuse the training one\n",
        "    # Ensure compute_dtype is compatible with your GPU for inference, often float16 is safe\n",
        "    inf_bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16, # Use float16 for inference compute\n",
        "        bnb_4bit_use_double_quant=False )\n",
        "    inf_model_kwargs = { \"quantization_config\": inf_bnb_config, \"device_map\": \"auto\",\n",
        "                         \"trust_remote_code\": True, \"attn_implementation\": \"eager\" }\n",
        "    if token: inf_model_kwargs[\"token\"] = token # Pass token for inference model load too\n",
        "    inf_model = AutoModelForCausalLM.from_pretrained(MODEL_ID, **inf_model_kwargs)\n",
        "\n",
        "    # Load the trained LoRA adapters\n",
        "    print(f\"Loading adapters from: {OUTPUT_DIR}\")\n",
        "    # PeftModel wraps the base model and applies the adapters\n",
        "    inf_model = PeftModel.from_pretrained(inf_model, OUTPUT_DIR)\n",
        "    print(\"Adapters loaded onto base model.\")\n",
        "\n",
        "    # Load the tokenizer again for inference\n",
        "    inf_tokenizer_kwargs = {\"trust_remote_code\": True}\n",
        "    if token: inf_tokenizer_kwargs[\"token\"] = token # Pass token for inference tokenizer load\n",
        "    inf_tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, **inf_tokenizer_kwargs)\n",
        "    if inf_tokenizer.pad_token is None: inf_tokenizer.pad_token = inf_tokenizer.eos_token\n",
        "    inf_tokenizer.padding_side = \"right\" # For generation, padding side should be 'right'\n",
        "\n",
        "    # Prepare a test prompt from the dataset's first example\n",
        "    # We need to extract just the instruction part for the prompt\n",
        "    test_formatted_text = dataset[0]['text']\n",
        "    inst_end_marker = \"[/INST]\"\n",
        "    prompt_end_index = test_formatted_text.find(inst_end_marker)\n",
        "\n",
        "    if prompt_end_index != -1:\n",
        "        # Extract only the part up to and including the instruction end marker\n",
        "        # This simulates giving the model an instruction to complete\n",
        "        prompt_for_model = test_formatted_text[:prompt_end_index + len(inst_end_marker)].strip()\n",
        "    else:\n",
        "        # Fallback in case the formatting wasn't perfect, try to reconstruct the prompt\n",
        "        # This might not be ideal if the formatting function changes\n",
        "        print(\"Warning: Could not find [/INST] marker in formatted text. Attempting to reconstruct prompt.\")\n",
        "        prompt_for_model = f\"<s>[INST] {dataset[0].get('instruction', '').strip()} [/INST]\"\n",
        "\n",
        "    # Ensure the prompt starts with the correct BOS token if not already present\n",
        "    if not prompt_for_model.startswith(inf_tokenizer.bos_token):\n",
        "         prompt_for_model = inf_tokenizer.bos_token + prompt_for_model\n",
        "\n",
        "    print(f\"\\nTest Prompt:\\n---\\n{prompt_for_model}\\n---\")\n",
        "\n",
        "    # Create a text generation pipeline using the adapted model and tokenizer\n",
        "    pipe = pipeline(task=\"text-generation\", model=inf_model, tokenizer=inf_tokenizer, max_new_tokens=150)\n",
        "\n",
        "    # Generate text based on the prompt\n",
        "    # The pipe will handle tokenizing the prompt, running inference, and decoding\n",
        "    result = pipe(prompt_for_model)\n",
        "\n",
        "    print(\"\\nGenerated Output:\")\n",
        "    # The result is a list of dictionaries. We take the text from the first result.\n",
        "    generated_text = result[0]['generated_text']\n",
        "\n",
        "    # Post-process the generated text to show only the generated response\n",
        "    # Find the end of the prompt in the generated text\n",
        "    prompt_end_in_generated = generated_text.find(prompt_for_model)\n",
        "    if prompt_end_in_generated != -1:\n",
        "      # Start of the generated response is after the prompt\n",
        "      answer_start_index = prompt_end_in_generated + len(prompt_for_model)\n",
        "      answer = generated_text[answer_start_index:].strip()\n",
        "\n",
        "      # Remove the final EOS token if it exists\n",
        "      if answer.endswith(inf_tokenizer.eos_token):\n",
        "          answer = answer[:-len(inf_tokenizer.eos_token)].strip()\n",
        "      print(answer)\n",
        "    else:\n",
        "      # If the prompt wasn't found in the generated text (shouldn't happen with pipeline),\n",
        "      # print the whole generation or a warning\n",
        "      print(\"Warning: Prompt not found in generated text. Printing full output:\")\n",
        "      print(generated_text)\n",
        "\n",
        "\n",
        "    # Print the expected output from the dataset for comparison\n",
        "    print(\"\\nExpected Output (from dataset):\")\n",
        "    print(dataset[0].get('output', 'N/A')) # Use .get for safety\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nError during inference testing: {e}\")\n",
        "\n",
        "print(\"\\n--- Script Finished ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "rCX6eYcEWTW5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCX6eYcEWTW5",
        "outputId": "b13d9ad4-43a3-458b-f9bc-eaf9ecc37069"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# # Go to YOLOv8 root folder\n",
        "# %cd /content/drive/MyDrive/\n",
        "\n",
        "# # Create folder\n",
        "# !mkdir Argentina\n",
        "\n",
        "# # Go to YOLOv8 root folder\n",
        "# %cd /content/drive/MyDrive/Argentina"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "IqUyCVNPWkId",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqUyCVNPWkId",
        "outputId": "057e60a0-ad78-4011-91bf-bb05fcedbc8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All items copied from /content/gemma-finetuned-adapters to /content/drive/MyDrive/DS-OLD/gemma-3-regular\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "source_folder = \"/content/gemma-finetuned-adapters\"\n",
        "destination_folder = \"/content/drive/MyDrive/DS-OLD/gemma-3-regular\"\n",
        "\n",
        "# Create destination folder and any parent directories if they don't exist\n",
        "os.makedirs(destination_folder, exist_ok=True)\n",
        "\n",
        "# Get all files and directories in the source folder\n",
        "for item in os.listdir(source_folder):\n",
        "    source_item = os.path.join(source_folder, item)\n",
        "    destination_item = os.path.join(destination_folder, item)\n",
        "\n",
        "    # If it's a directory, copy the directory\n",
        "    if os.path.isdir(source_item):\n",
        "        # Check if destination directory exists\n",
        "        if os.path.exists(destination_item):\n",
        "            shutil.rmtree(destination_item)  # Remove existing directory\n",
        "        shutil.copytree(source_item, destination_item)\n",
        "    # If it's a file, copy the file\n",
        "    else:\n",
        "        shutil.copy2(source_item, destination_item)\n",
        "\n",
        "print(f\"All items copied from {source_folder} to {destination_folder}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "yDRKaADdWZLK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDRKaADdWZLK",
        "outputId": "b0b54902-2066-4f70-8536-689e07e35bea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Directory already exists: /content/drive/MyDrive/DS-OLD/\n",
            "/content/drive/MyDrive/DS-OLD\n"
          ]
        }
      ],
      "source": [
        "# Go to YOLOv8 root folder\n",
        "import os\n",
        "\n",
        "target_dir = \"/content/drive/MyDrive/DS-OLD/\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(target_dir):\n",
        "    os.makedirs(target_dir)\n",
        "    print(f\"Created directory: {target_dir}\")\n",
        "else:\n",
        "    print(f\"Directory already exists: {target_dir}\")\n",
        "\n",
        "# Change to the directory\n",
        "%cd \"{target_dir}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zf27XMJvdIau",
      "metadata": {
        "id": "zf27XMJvdIau"
      },
      "source": [
        "# Continue Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "kFi8KrW9UN7v",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFi8KrW9UN7v",
        "outputId": "fb58ef6d-5286-4c30-8907-3163ccbb568b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully removed folder: /content/gemma-finetuned-adapters/checkpoint-100\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "def remove_folder(folder_path):\n",
        "    \"\"\"\n",
        "    Remove a folder and all its contents\n",
        "\n",
        "    Args:\n",
        "        folder_path: Path to the folder you want to remove\n",
        "    \"\"\"\n",
        "    if os.path.exists(folder_path):\n",
        "        if os.path.isdir(folder_path):\n",
        "            try:\n",
        "                # Remove directory and all its contents\n",
        "                shutil.rmtree(folder_path)\n",
        "                print(f\"Successfully removed folder: {folder_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error removing folder: {e}\")\n",
        "        else:\n",
        "            print(f\"The path exists but is not a directory: {folder_path}\")\n",
        "    else:\n",
        "        print(f\"Folder does not exist: {folder_path}\")\n",
        "\n",
        "# Example usage\n",
        "remove_folder('/content/gemma-finetuned-adapters/checkpoint-100')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ty_KlGKmXA8h",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b90f3c73749f4fd796348d59b3b6ade2",
            "90cae1d2990d429d9c52e57149c042aa",
            "e06253cb6854457b87d41a85f14da5dd",
            "0f97e57823304ba983b9295120963b6e",
            "471806eefc9845c4a1a5d3af312ef987",
            "d795851fef7c41d2935179c58e3012d0",
            "7647b89f4c1d4ffd89cd29b6013766f4",
            "1a2a2949de1641f8bc8703e45088cbdc",
            "285c8a4e7a1d450b8fa04ca55c6e2438",
            "0d4bccce43e844d281a353096503d215",
            "c28e314865044facb9b429d27c348c10"
          ]
        },
        "id": "Ty_KlGKmXA8h",
        "outputId": "f1142ed2-8f1f-4fea-df78-50906e2126c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "Device name: Tesla T4\n",
            "GPU does NOT support bfloat16. Setting bf16=False, fp16=True.\n",
            "Loading tokenizer for model: google/gemma-3-4b-it\n",
            "Loading base model: google/gemma-3-4b-it with QLoRA config\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b90f3c73749f4fd796348d59b3b6ade2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset from: llm_training_data_claude-22.jsonl\n",
            "Dataset loaded successfully. Number of examples: 596\n",
            "Formatting dataset...\n",
            "Dataset formatted and filtered.\n",
            "Example formatted text:\n",
            "<s>[INST] Extract the following information from the given text of driving license: Address, First_Name, Height, Last_Name, Sex, State\n",
            "\n",
            "Text: Driver\n",
            "ucend - No\n",
            "j U 216 *\n",
            "Licence\n",
            "JONES MARGARET CHERYL\n",
            "50 SMITH ST\n",
            "SMITHVILLE QLD 9999\n",
            "itninre\n",
            "aply\n",
            "26-03-80 F 170\n",
            "01-07-2006 30-06-2011\n",
            "Queensland\n",
            "Goverament\n",
            "eens and Transport [/INST] {\"Last_Name\": \"JONES\", \"First_Name\": \"MARGARET CHERYL\", \"Address\": \"50 SMITH ST\\nSMITHVILLE QLD 9999\", \"Sex\": \"F\", \"Height\": \"170\", \"State\": \"Queensland\"} </s>\n",
            "Initializing SFTTrainer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:323: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3335' max='74500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 3335/74500 14:56 < 76:04:05, 0.26 it/s, Epoch 22.38/500]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>3125</td>\n",
              "      <td>0.255000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3150</td>\n",
              "      <td>0.268200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3175</td>\n",
              "      <td>0.305500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>0.282400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3225</td>\n",
              "      <td>0.304100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3250</td>\n",
              "      <td>0.299900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3275</td>\n",
              "      <td>0.319000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>0.264300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3325</td>\n",
              "      <td>0.267000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f34f2-2bf3c3270477761d7de3b527;c73bbf80-9854-4704-b2c2-0001dd404d27)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-3-4b-it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-3-4b-it - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f3670-19be74286f9268e006d9f67f;4e0a6fa6-56a7-4e79-8823-61ae86591b4b)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-3-4b-it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-3-4b-it - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "# from huggingface_hub import notebook_login # Uncomment if using notebook login\n",
        "\n",
        "# --- Configuration ---\n",
        "MODEL_ID = \"google/gemma-3-4b-it\"\n",
        "DATASET_PATH = \"llm_training_data_claude-22.jsonl\" # Replace with your dataset path\n",
        "OUTPUT_DIR = \"gemma-finetuned-adapters\" # Will be created fresh\n",
        "\n",
        "# --- Authentication ---\n",
        "# IMPORTANT: Replace the placeholder token below with your actual Hugging Face token.\n",
        "# Using a manual token string like this is ONE way to authenticate.\n",
        "# The recommended way is usually `huggingface-cli login` or `notebook_login()`.\n",
        "# Choose ONE method only. If you set the token here, you don't need to run login commands.\n",
        "token = \"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\" # Replace with your actual token from HF settings\n",
        "\n",
        "# Make sure you are logged in via `huggingface-cli login` OR uncomment and run:\n",
        "# notebook_login() # Only if you are in a notebook and token is None\n",
        "\n",
        "# --- QLoRA Configuration ---\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16, # Set based on GPU check below\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "# --- LoRA Configuration ---\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    target_modules=[ \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\" ],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# --- Training Arguments ---\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=500,             # Starting with 3 epochs (adjust as needed)\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4, # Effective batch size = per_device_train_batch_size * gradient_accumulation_steps\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,                 # Save checkpoint every 100 steps (adjust as needed)\n",
        "    logging_steps=25,               # Log every 25 steps (adjust as needed)\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    fp16=False,                     # Will be set based on GPU check\n",
        "    bf16=False,                     # Will be set based on GPU check\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,                   # Train for num_train_epochs, or set a positive value for max steps\n",
        "    warmup_ratio=0.03,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "# --- Check GPU Availability & Set Precisions ---\n",
        "if not torch.cuda.is_available():\n",
        "    raise SystemError(\"CUDA is not available.\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
        "if torch.cuda.get_device_capability(0)[0] >= 8:\n",
        "    print(\"GPU supports bfloat16. Setting bf16=True, fp16=False.\")\n",
        "    training_arguments.bf16 = True\n",
        "    training_arguments.fp16 = False\n",
        "    bnb_config.bnb_4bit_compute_dtype = torch.bfloat16\n",
        "else:\n",
        "    print(\"GPU does NOT support bfloat16. Setting bf16=False, fp16=True.\")\n",
        "    training_arguments.bf16 = False\n",
        "    training_arguments.fp16 = True\n",
        "    bnb_config.bnb_4bit_compute_dtype = torch.float16\n",
        "\n",
        "\n",
        "# --- Load Tokenizer and Model ---\n",
        "print(f\"Loading tokenizer for model: {MODEL_ID}\")\n",
        "# This dictionary correctly adds the 'token' argument if the token variable is set\n",
        "tokenizer_kwargs = {\"trust_remote_code\": True}\n",
        "if token: tokenizer_kwargs[\"token\"] = token\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, **tokenizer_kwargs)\n",
        "if tokenizer.pad_token is None:\n",
        "    print(\"Setting pad_token = eos_token\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(f\"Loading base model: {MODEL_ID} with QLoRA config\")\n",
        "# This dictionary correctly adds the 'token' argument if the token variable is set\n",
        "model_kwargs = {\n",
        "    \"quantization_config\": bnb_config,\n",
        "    \"device_map\": \"auto\",\n",
        "    \"trust_remote_code\": True,\n",
        "    \"attn_implementation\": \"eager\", # Use eager attention for Gemma3 (Recommended by HF docs)\n",
        "}\n",
        "if token: model_kwargs[\"token\"] = token\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, **model_kwargs)\n",
        "\n",
        "# --- Load Dataset ---\n",
        "print(f\"Loading dataset from: {DATASET_PATH}\")\n",
        "try:\n",
        "    # Assuming the dataset is a JSON Lines file\n",
        "    dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
        "    print(f\"Dataset loaded successfully. Number of examples: {len(dataset)}\")\n",
        "except FileNotFoundError:\n",
        "     print(f\"Error: Dataset file not found at {DATASET_PATH}\"); exit()\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\"); exit()\n",
        "\n",
        "# --- Preprocess Dataset ---\n",
        "# This function formats each data sample into a single text string\n",
        "# suitable for Causal LM training, using the Gemma-specific instruction format.\n",
        "def format_instruction(sample):\n",
        "    # Extract instruction and output, handle potential None or empty strings\n",
        "    instruction = sample.get('instruction', '') or ''\n",
        "    output = sample.get('output', '') or ''\n",
        "\n",
        "    # Clean up potential leftover tokens/whitespace from data source\n",
        "    instruction = instruction.strip().removeprefix(\"<s>\").removesuffix(\"</s>\").strip()\n",
        "    output = output.strip().removeprefix(\"<s>\").removesuffix(\"</s>\").strip()\n",
        "\n",
        "    # Format according to Gemma's instruction format: <s>[INST] Instruction [/INST] Output </s>\n",
        "    # Ensure there's content before formatting\n",
        "    if not instruction:\n",
        "        print(f\"Warning: Skipping sample with no instruction: {sample}\")\n",
        "        return {\"text\": \"\"} # Return empty text for samples to be filtered out\n",
        "    if not output:\n",
        "         print(f\"Warning: Sample with instruction but no output: {sample}\")\n",
        "         # Optionally format without output if you want to train on prompts only\n",
        "         # For this setup (SFT), we usually expect prompt-response pairs\n",
        "         # Returning empty text will effectively skip this sample for training\n",
        "         return {\"text\": \"\"}\n",
        "\n",
        "\n",
        "    return f\"<s>[INST] {instruction} [/INST] {output} </s>\"\n",
        "\n",
        "print(\"Formatting dataset...\")\n",
        "try:\n",
        "    # Apply the formatting function to each sample\n",
        "    dataset = dataset.map(lambda sample: {\"text\": format_instruction(sample)})\n",
        "    # Filter out samples that resulted in empty text (e.g., missing instruction/output)\n",
        "    dataset = dataset.filter(lambda sample: sample[\"text\"] != \"\")\n",
        "\n",
        "    print(\"Dataset formatted and filtered.\")\n",
        "    if not dataset: print(\"Warning: Processed dataset is empty. Check your data file and formatting function.\"); exit()\n",
        "    print(f\"Example formatted text:\\n{dataset[0]['text']}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error formatting dataset: {e}\"); exit()\n",
        "\n",
        "# --- Initialize Trainer ---\n",
        "print(\"Initializing SFTTrainer...\")\n",
        "# SFTTrainer handles wrapping the base model with LoRA adapters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=lora_config,\n",
        "    dataset_text_field=\"text\", # The column containing the formatted text\n",
        "    max_seq_length=512,        # Maximum sequence length for training\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=False,             # Set to True for more efficient training if max_seq_length varies widely\n",
        ")\n",
        "\n",
        "# --- Train the Model ---\n",
        "print(\"Starting training...\")\n",
        "try:\n",
        "    # If you had a previous checkpoint and want to resume, uncomment the line below\n",
        "    # trainer.train(resume_from_checkpoint=True)\n",
        "    trainer.train(resume_from_checkpoint=True) # Resuming training\n",
        "    print(\"Training finished.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during training: {e}\"); exit()\n",
        "\n",
        "# --- Save the Trained Adapters ---\n",
        "# SFTTrainer automatically saves the model/adapters to output_dir\n",
        "print(f\"Saving LoRA adapters to: {OUTPUT_DIR}\")\n",
        "trainer.save_model()\n",
        "print(\"Adapters saved successfully.\")\n",
        "\n",
        "# --- Optional: Clean up memory ---\n",
        "# Free up GPU memory by deleting model and trainer objects\n",
        "print(\"Cleaning up memory...\")\n",
        "if 'model' in locals(): del model\n",
        "if 'trainer' in locals(): del trainer\n",
        "# Additional manual cleanup for objects potentially holding onto memory\n",
        "if 'tokenizer' in locals() and hasattr(tokenizer, 'model'): del tokenizer.model\n",
        "if 'pipe' in locals() and hasattr(pipe, 'model'): del pipe.model\n",
        "if 'inf_model' in locals(): del inf_model\n",
        "torch.cuda.empty_cache() # Clear cached memory\n",
        "print(\"Memory cleaned.\")\n",
        "\n",
        "# --- Optional: Test Inference ---\n",
        "# Load the base model again and merge the adapters for inference\n",
        "print(\"\\n--- Testing Inference with Adapters ---\")\n",
        "# Set logging verbosity to critical to avoid excessive output from transformers\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "try:\n",
        "    # Load base model for inference - use float16 compute dtype typically for inference\n",
        "    print(\"Loading base model for inference...\")\n",
        "    # Use a fresh BitsAndBytesConfig for inference if needed, or reuse the training one\n",
        "    # Ensure compute_dtype is compatible with your GPU for inference, often float16 is safe\n",
        "    inf_bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16, # Use float16 for inference compute\n",
        "        bnb_4bit_use_double_quant=False )\n",
        "    inf_model_kwargs = { \"quantization_config\": inf_bnb_config, \"device_map\": \"auto\",\n",
        "                         \"trust_remote_code\": True, \"attn_implementation\": \"eager\" }\n",
        "    if token: inf_model_kwargs[\"token\"] = token # Pass token for inference model load too\n",
        "    inf_model = AutoModelForCausalLM.from_pretrained(MODEL_ID, **inf_model_kwargs)\n",
        "\n",
        "    # Load the trained LoRA adapters\n",
        "    print(f\"Loading adapters from: {OUTPUT_DIR}\")\n",
        "    # PeftModel wraps the base model and applies the adapters\n",
        "    inf_model = PeftModel.from_pretrained(inf_model, OUTPUT_DIR)\n",
        "    print(\"Adapters loaded onto base model.\")\n",
        "\n",
        "    # Load the tokenizer again for inference\n",
        "    inf_tokenizer_kwargs = {\"trust_remote_code\": True}\n",
        "    if token: inf_tokenizer_kwargs[\"token\"] = token # Pass token for inference tokenizer load\n",
        "    inf_tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, **inf_tokenizer_kwargs)\n",
        "    if inf_tokenizer.pad_token is None: inf_tokenizer.pad_token = inf_tokenizer.eos_token\n",
        "    inf_tokenizer.padding_side = \"right\" # For generation, padding side should be 'right'\n",
        "\n",
        "    # Prepare a test prompt from the dataset's first example\n",
        "    # We need to extract just the instruction part for the prompt\n",
        "    test_formatted_text = dataset[0]['text']\n",
        "    inst_end_marker = \"[/INST]\"\n",
        "    prompt_end_index = test_formatted_text.find(inst_end_marker)\n",
        "\n",
        "    if prompt_end_index != -1:\n",
        "        # Extract only the part up to and including the instruction end marker\n",
        "        # This simulates giving the model an instruction to complete\n",
        "        prompt_for_model = test_formatted_text[:prompt_end_index + len(inst_end_marker)].strip()\n",
        "    else:\n",
        "        # Fallback in case the formatting wasn't perfect, try to reconstruct the prompt\n",
        "        # This might not be ideal if the formatting function changes\n",
        "        print(\"Warning: Could not find [/INST] marker in formatted text. Attempting to reconstruct prompt.\")\n",
        "        prompt_for_model = f\"<s>[INST] {dataset[0].get('instruction', '').strip()} [/INST]\"\n",
        "\n",
        "    # Ensure the prompt starts with the correct BOS token if not already present\n",
        "    if not prompt_for_model.startswith(inf_tokenizer.bos_token):\n",
        "         prompt_for_model = inf_tokenizer.bos_token + prompt_for_model\n",
        "\n",
        "    print(f\"\\nTest Prompt:\\n---\\n{prompt_for_model}\\n---\")\n",
        "\n",
        "    # Create a text generation pipeline using the adapted model and tokenizer\n",
        "    pipe = pipeline(task=\"text-generation\", model=inf_model, tokenizer=inf_tokenizer, max_new_tokens=150)\n",
        "\n",
        "    # Generate text based on the prompt\n",
        "    # The pipe will handle tokenizing the prompt, running inference, and decoding\n",
        "    result = pipe(prompt_for_model)\n",
        "\n",
        "    print(\"\\nGenerated Output:\")\n",
        "    # The result is a list of dictionaries. We take the text from the first result.\n",
        "    generated_text = result[0]['generated_text']\n",
        "\n",
        "    # Post-process the generated text to show only the generated response\n",
        "    # Find the end of the prompt in the generated text\n",
        "    prompt_end_in_generated = generated_text.find(prompt_for_model)\n",
        "    if prompt_end_in_generated != -1:\n",
        "      # Start of the generated response is after the prompt\n",
        "      answer_start_index = prompt_end_in_generated + len(prompt_for_model)\n",
        "      answer = generated_text[answer_start_index:].strip()\n",
        "\n",
        "      # Remove the final EOS token if it exists\n",
        "      if answer.endswith(inf_tokenizer.eos_token):\n",
        "          answer = answer[:-len(inf_tokenizer.eos_token)].strip()\n",
        "      print(answer)\n",
        "    else:\n",
        "      # If the prompt wasn't found in the generated text (shouldn't happen with pipeline),\n",
        "      # print the whole generation or a warning\n",
        "      print(\"Warning: Prompt not found in generated text. Printing full output:\")\n",
        "      print(generated_text)\n",
        "\n",
        "\n",
        "    # Print the expected output from the dataset for comparison\n",
        "    print(\"\\nExpected Output (from dataset):\")\n",
        "    print(dataset[0].get('output', 'N/A')) # Use .get for safety\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nError during inference testing: {e}\")\n",
        "\n",
        "print(\"\\n--- Script Finished ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R6NBMBMQbd6a",
      "metadata": {
        "id": "R6NBMBMQbd6a"
      },
      "source": [
        "# Zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "akXjL6FBbdPg",
      "metadata": {
        "id": "akXjL6FBbdPg"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Function to zip a folder without downloading\n",
        "def zip_folder(folder_path, output_zip_name):\n",
        "    \"\"\"\n",
        "    Zip an entire folder in Colab\n",
        "\n",
        "    Args:\n",
        "        folder_path: Path to the folder you want to zip\n",
        "        output_zip_name: Name for the output zip file\n",
        "    \"\"\"\n",
        "    # Create a ZipFile object\n",
        "    with zipfile.ZipFile(output_zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        # Walk through the directory\n",
        "        for root, dirs, files in os.walk(folder_path):\n",
        "            for file in files:\n",
        "                # Create the full file path\n",
        "                file_path = os.path.join(root, file)\n",
        "                # Calculate path within the zip file\n",
        "                arcname = os.path.relpath(file_path, os.path.dirname(folder_path))\n",
        "                # Add file to zip\n",
        "                zipf.write(file_path, arcname)\n",
        "\n",
        "    print(f\"Folder '{folder_path}' has been zipped to '{output_zip_name}'\")\n",
        "\n",
        "# Example usage\n",
        "zip_folder('/content/gemma-finetuned-adapters', 'models.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dL7yPGtrhmcf",
      "metadata": {
        "id": "dL7yPGtrhmcf"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92db474c-b749-453a-b086-b86588d07658",
      "metadata": {
        "id": "92db474c-b749-453a-b086-b86588d07658",
        "outputId": "48f5d00a-a444-4544-fd10-ddf827fb6635"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU does NOT support bfloat16. Using float16.\n",
            "Loading model onto device: cuda:0\n",
            "Loading base model: google/gemma-3-1b-it\n",
            "Base model loaded.\n",
            "Loading tokenizer: google/gemma-3-1b-it\n",
            "Tokenizer loaded.\n",
            "Loading LoRA adapters from: ./gemma-finetuned-adapters\n",
            "LoRA adapters loaded and merged onto base model.\n",
            "Inference pipeline ready.\n",
            "\n",
            "--- Running Inference ---\n",
            "Prompt:\n",
            "<s>[INST] Extract information from the given text of driving license: California\n",
            "        DRIVER LICENSe\n",
            "        dl 11234568\n",
            "        CLASS C\n",
            "        EXP 08/31/2014\n",
            "        END NONE\n",
            "        LNCARDHOLDER FNIMA\n",
            "        2570 24TH STREET ANYTOWN, CA 95818\n",
            "        doB 08/31/1977 RSTR NONE\n",
            "        08311977\n",
            "        VETERAN\n",
            "        Cordhslde\n",
            "        SEX F HGT 5'-05\"\n",
            "        HAIR BRN WGT 125 lb\n",
            "        EYES BRN\n",
            "        DD 00/00/0000NNNAN/ANFD/YY\n",
            "        ISS 08/31/2009 [/INST]\n",
            "\n",
            "Generated Output:\n",
            "{\"License_Class\": \"C\", \"License_Number\": \"11234568\", \"End\": \"NONE\", \"Country\": \"USA\", \"State\": \"California\", \"Class_Name\": \"VETERAN\", \"Driver_Name\": \"CORDHSlde\", \"Birth_Place\": \"2570 24TH STREET ANYTOWN, CA 95818\", \"Sex\": \"F\", \"Height\": \"5'-\n",
            "\n",
            "--- Inference Finished ---\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import PeftModel\n",
        "import os\n",
        "\n",
        "# --- Configuration (Should match your training script) ---\n",
        "\n",
        "# Base Model identifier\n",
        "MODEL_ID = \"google/gemma-3-1b-it\"\n",
        "\n",
        "# Path where your trained LoRA adapters were saved\n",
        "ADAPTER_PATH = \"./gemma-finetuned-adapters\" # Use the same OUTPUT_DIR from training\n",
        "\n",
        "# Optional: Authentication token if required by the base model\n",
        "# Define 'token' same way as in training if needed (CLI login, notebook_login, or string)\n",
        "token = None # Or your \"hf_...\" token string if you used that method\n",
        "\n",
        "# --- Determine Compute dtype based on GPU (Same logic as training) ---\n",
        "if torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8:\n",
        "    compute_dtype = torch.bfloat16\n",
        "    print(\"GPU supports bfloat16. Using bfloat16.\")\n",
        "else:\n",
        "    compute_dtype = torch.float16\n",
        "    print(\"GPU does NOT support bfloat16. Using float16.\")\n",
        "\n",
        "# --- QLoRA Configuration for Inference Loading ---\n",
        "# Use the same quantization settings as training\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype, # Use determined compute dtype\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "# --- Check GPU Availability ---\n",
        "if not torch.cuda.is_available():\n",
        "    raise SystemError(\"CUDA is not available. Inference requires a GPU for this setup.\")\n",
        "print(f\"Loading model onto device: cuda:{torch.cuda.current_device()}\")\n",
        "\n",
        "\n",
        "# --- Load the Base Model with Quantization ---\n",
        "print(f\"Loading base model: {MODEL_ID}\")\n",
        "model_kwargs = {\n",
        "    \"quantization_config\": bnb_config,\n",
        "    \"device_map\": \"auto\", # Load onto available GPU(s) automatically\n",
        "    \"trust_remote_code\": True,\n",
        "}\n",
        "if token:\n",
        "    model_kwargs[\"token\"] = token\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    **model_kwargs\n",
        ")\n",
        "print(\"Base model loaded.\")\n",
        "\n",
        "# --- Load the Tokenizer ---\n",
        "print(f\"Loading tokenizer: {MODEL_ID}\")\n",
        "tokenizer_kwargs = {\"trust_remote_code\": True}\n",
        "if token:\n",
        "    tokenizer_kwargs[\"token\"] = token\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, **tokenizer_kwargs)\n",
        "\n",
        "# Set pad token if needed (important for batching/padding during generation if used)\n",
        "if tokenizer.pad_token is None:\n",
        "    print(\"Setting pad_token = eos_token\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Consistent with training\n",
        "print(\"Tokenizer loaded.\")\n",
        "\n",
        "# --- Load the LoRA Adapters onto the Base Model ---\n",
        "print(f\"Loading LoRA adapters from: {ADAPTER_PATH}\")\n",
        "# This automatically merges the adapters onto the base model layers\n",
        "inference_model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
        "print(\"LoRA adapters loaded and merged onto base model.\")\n",
        "\n",
        "# --- Prepare for Inference ---\n",
        "logging.set_verbosity(logging.CRITICAL) # Suppress verbose generation messages\n",
        "pipe = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=inference_model, # Use the model with adapters loaded\n",
        "    tokenizer=tokenizer,\n",
        "    # device=0 # pipeline usually handles device placement with device_map=\"auto\"\n",
        ")\n",
        "print(\"Inference pipeline ready.\")\n",
        "\n",
        "# --- Example Inference ---\n",
        "# **IMPORTANT**: Format your prompt EXACTLY like you did for training!\n",
        "test_instruction = '''Extract information from the given text of driving license: California\n",
        "        DRIVER LICENSe\n",
        "        dl 11234568\n",
        "        CLASS C\n",
        "        EXP 08/31/2014\n",
        "        END NONE\n",
        "        LNCARDHOLDER FNIMA\n",
        "        2570 24TH STREET ANYTOWN, CA 95818\n",
        "        doB 08/31/1977 RSTR NONE\n",
        "        08311977\n",
        "        VETERAN\n",
        "        Cordhslde\n",
        "        SEX F HGT 5'-05\"\n",
        "        HAIR BRN WGT 125 lb\n",
        "        EYES BRN\n",
        "        DD 00/00/0000NNNAN/ANFD/YY\n",
        "        ISS 08/31/2009\n",
        "'''\n",
        "\n",
        "# Replace with your actual test instruction text\n",
        "prompt = f\"<s>[INST] {test_instruction.strip()} [/INST]\"\n",
        "\n",
        "print(f\"\\n--- Running Inference ---\")\n",
        "print(f\"Prompt:\\n{prompt}\")\n",
        "\n",
        "# Set generation parameters\n",
        "# max_new_tokens determines how many tokens to generate *after* the prompt\n",
        "generation_args = {\n",
        "    \"max_new_tokens\": 100, # Adjust as needed\n",
        "    \"do_sample\": True,    # Whether to sample; False means greedy decoding\n",
        "    \"temperature\": 0.7, # Controls randomness (lower = more deterministic) - use if do_sample=True\n",
        "    \"top_k\": 50,        # Considers top_k tokens for sampling - use if do_sample=True\n",
        "    \"top_p\": 0.95,      # Uses nucleus sampling - use if do_sample=True\n",
        "    \"eos_token_id\": tokenizer.eos_token_id,\n",
        "    \"pad_token_id\": tokenizer.eos_token_id, # Often set same as EOS for generation\n",
        "}\n",
        "\n",
        "try:\n",
        "    outputs = pipe(prompt, **generation_args)\n",
        "\n",
        "    print(\"\\nGenerated Output:\")\n",
        "    # Extract only the generated part (after the prompt)\n",
        "    generated_text = outputs[0]['generated_text']\n",
        "    inst_end_marker = \"[/INST]\"\n",
        "    answer_start_index = generated_text.find(inst_end_marker)\n",
        "    if answer_start_index != -1:\n",
        "        answer = generated_text[answer_start_index + len(inst_end_marker):].strip()\n",
        "        # Remove potential EOS token at the end if needed\n",
        "        if answer.endswith(tokenizer.eos_token):\n",
        "            answer = answer[:-len(tokenizer.eos_token)].strip()\n",
        "        print(answer)\n",
        "    else:\n",
        "       print(generated_text) # Print full output if prompt marker wasn't found\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during generation: {e}\")\n",
        "\n",
        "print(\"\\n--- Inference Finished ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2e6ffe3-2959-48e9-9201-4b1426d054ad",
      "metadata": {
        "id": "b2e6ffe3-2959-48e9-9201-4b1426d054ad"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01ac06dc070d45db8de78a8649b4ad0e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "028af4ac533a4b9e8df1a68033188c55": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "037aa56ec52046b3b38c28cc435ef656": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04e9be96f5a746ce98fdab6511102b82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_949f7cfe8ec547c38bbcb35510d705a6",
            "placeholder": "​",
            "style": "IPY_MODEL_a08d16bde9ef4556a7382fc41a2a7a5b",
            "value": " 899/899 [00:00&lt;00:00, 71.4kB/s]"
          }
        },
        "09d51245ccf44088ae0f5668e9d3d9d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eaf3c23b242344ee85277bd3e9fafcee",
              "IPY_MODEL_c3c9d06ea616458d8430d037d566bce9",
              "IPY_MODEL_50413b245f56427e878745ea609f9439"
            ],
            "layout": "IPY_MODEL_83bd570fb7934690acbebe9a862ce252"
          }
        },
        "0b545b82a615456fa74b8b5b0e19d6dc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c5e22e505ef42eabdc846ccedf8584a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0cc85a59f30140128fc04c065e0c526c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0d4bccce43e844d281a353096503d215": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0da5230e1da3486bb0b8556e00499daa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e1c743b772e42aa9766eb7c34e92bec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f97e57823304ba983b9295120963b6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d4bccce43e844d281a353096503d215",
            "placeholder": "​",
            "style": "IPY_MODEL_c28e314865044facb9b429d27c348c10",
            "value": " 2/2 [01:04&lt;00:00, 31.59s/it]"
          }
        },
        "114e4083103f4cb591176b004cc36121": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a173419e200b4d158c3b4e54e9685cb5",
            "placeholder": "​",
            "style": "IPY_MODEL_6617ddad3fd049e8a8f22615729326e5",
            "value": " 596/0 [00:00&lt;00:00, 7108.08 examples/s]"
          }
        },
        "12389ee735e94d938144744d4e56c06a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "133dda9cb90f4511b48b1c97e7dcdd00": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "187d423fc4034cc0a10a1232b0c5c4e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a2a2949de1641f8bc8703e45088cbdc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b0aece87c934f1fb014f26307ba5b07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc1195417f7142bba8b40a67d93472fa",
            "placeholder": "​",
            "style": "IPY_MODEL_e2d7c11476cb47eb8d57d0295a4ec753",
            "value": "config.json: 100%"
          }
        },
        "1daacc7f22044f068e98acc66f25e28b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e4e9fd5d5f14190934caf79cbe41dbd",
            "max": 1999811208,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_46f10eec3f4e4c62a214bace4db4508d",
            "value": 1999811208
          }
        },
        "1dc96f8f5dbc4d94ad6c7e781a22e73f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1de63a00515d46b3bd5a8137ad002ce6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e863cf5eb8846b7ba03975ec3a5783f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f828456740945bd8a785893599a91d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22d8a5705a6c4b6fa97bd6160f151083": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_852fc02a5e914b4cb662b7aaf0ccfe81",
            "placeholder": "​",
            "style": "IPY_MODEL_838a06168cf24449b180936fbfe1c232",
            "value": "model.safetensors: 100%"
          }
        },
        "261508f501d5412ea7de5a41ee90b80a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_133dda9cb90f4511b48b1c97e7dcdd00",
            "max": 899,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d342f2ffe7c48fd99cf53bba7353692",
            "value": 899
          }
        },
        "273afce93ff84324ac5a36ae7aa53bfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d054fc604d9c4f0fb26de1fc62fb4f73",
            "placeholder": "​",
            "style": "IPY_MODEL_3b8d0ee4cc4b46878fdaf9bd041ab2c5",
            "value": " 662/662 [00:00&lt;00:00, 53.6kB/s]"
          }
        },
        "277d08131e074dfebf6ddf1c718cb4b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "285c8a4e7a1d450b8fa04ca55c6e2438": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "292af84207384707b25b29f54d860bc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2963ff3893d34c87817b1ccfb0c5afaf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d2abb9d1bae49b0a033a9f4bb3384c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33daba5658ec4badb67146e8df10f73a",
            "placeholder": "​",
            "style": "IPY_MODEL_b105ae15a27340c8adc6c4faf4d0ead0",
            "value": " 596/596 [00:00&lt;00:00, 1737.95 examples/s]"
          }
        },
        "2d342f2ffe7c48fd99cf53bba7353692": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2f80ebb78f094b12ac800662ef02a49f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "33718572ba5e48f5aa15f2cb19911c99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "339ae78fe88e4931b48149b466bf7a97": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "33daba5658ec4badb67146e8df10f73a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "368bd9a9ade0454d93860fc5028d934d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf15706bdd2343969d3e13c9209cd1b0",
            "placeholder": "​",
            "style": "IPY_MODEL_0c5e22e505ef42eabdc846ccedf8584a",
            "value": "generation_config.json: 100%"
          }
        },
        "3a9f6313b39841c7918386895f37cde0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b8d0ee4cc4b46878fdaf9bd041ab2c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e46f6e1448748488f21d91d8137afb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41accdd5411c4a6eae76697fec3125a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4499de05ebff4fb98d1722f43c7e64fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46f10eec3f4e4c62a214bace4db4508d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "471806eefc9845c4a1a5d3af312ef987": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c10446103644f0bac03cc66dc30a475": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c99c03f2f0494ffa8e6593aaf66f4bbe",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_339ae78fe88e4931b48149b466bf7a97",
            "value": 2
          }
        },
        "4e0bad8e4cea465d9b7edac93e7d0456": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50413b245f56427e878745ea609f9439": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee8f3d949ea64716b46b2f861113c5b8",
            "placeholder": "​",
            "style": "IPY_MODEL_3e46f6e1448748488f21d91d8137afb3",
            "value": " 4.69M/4.69M [00:00&lt;00:00, 87.8MB/s]"
          }
        },
        "5112ac84b10a4446b5903c35e41dfcee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "511c7b16b3614cd081adf400fd6551ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cee91c294cb4c2686fc113c139a1bc0",
            "placeholder": "​",
            "style": "IPY_MODEL_292af84207384707b25b29f54d860bc5",
            "value": "Generating train split: "
          }
        },
        "51f939af465d455c988a11a5415523b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2963ff3893d34c87817b1ccfb0c5afaf",
            "placeholder": "​",
            "style": "IPY_MODEL_90a13d3617f846be889454d0908fba16",
            "value": "tokenizer.json: 100%"
          }
        },
        "52f5e9963f8642cea5bfb9acfff2e020": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5339b7456fc843019310edffb9b8da15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5f8bf04f2ad4737b96686b811479433",
            "placeholder": "​",
            "style": "IPY_MODEL_ab401dedd62b4d92a4ad8e954e2ea11d",
            "value": " 215/215 [00:00&lt;00:00, 23.5kB/s]"
          }
        },
        "549a07be71654c59ba8f6ae295088f6a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "55d1cc4c70944200a0f4fb8a52afa94f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55e26b0c25a84ca78b50fc728156db11": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "578ef4ea74a94ec383cb0418a3dfe08d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "586692bc26b7478a98c17e405b21e038": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58c3d53cdeb9459395233b3dd1023db3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0df088668624a0cbb0874b3c9ace1d5",
            "placeholder": "​",
            "style": "IPY_MODEL_cd0529ea983149bb9ee6cba5f33d8898",
            "value": " 2.00G/2.00G [00:07&lt;00:00, 251MB/s]"
          }
        },
        "59975630f23f40a3a46de2eea39f3cc7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d9143426760482e932651002cdd665d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8190b6d1997f4e1c89773265f0392fba",
              "IPY_MODEL_4c10446103644f0bac03cc66dc30a475",
              "IPY_MODEL_9ff35b4a29f241fd8bbf1c166b0b5ca9"
            ],
            "layout": "IPY_MODEL_0e1c743b772e42aa9766eb7c34e92bec"
          }
        },
        "5dae7f443e6a4bf58341e40bbb92473d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6113e27f1309421f84206cafbe358fb5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6617ddad3fd049e8a8f22615729326e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66620d5e2d6d42278df274790a5140ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b0b7d968c16b4060b8a4e508a02ac372",
              "IPY_MODEL_75af803c409241e7b2e1ca55587e8b1d",
              "IPY_MODEL_b7ee3c63932743e0817fd36ec9b8ef0d"
            ],
            "layout": "IPY_MODEL_1e863cf5eb8846b7ba03975ec3a5783f"
          }
        },
        "67f58c69ee7548c5853c80acbdefe497": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6bc32dc487504058909d4bc788d622e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f828456740945bd8a785893599a91d4",
            "max": 596,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1dc96f8f5dbc4d94ad6c7e781a22e73f",
            "value": 596
          }
        },
        "6f335294b4974233b8a237ef617ec3a4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f48fd9c41de4eb8a7884aafd2f63748": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b545b82a615456fa74b8b5b0e19d6dc",
            "placeholder": "​",
            "style": "IPY_MODEL_52f5e9963f8642cea5bfb9acfff2e020",
            "value": "Map: 100%"
          }
        },
        "704a46879a3e42629a1d36860efd7e79": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7355a3104c47476f8b7e6b9f42e533f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73afcd7c0d784f439612cc45265d3d02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebe59296438c4dd691f307d11ad5b7d4",
            "placeholder": "​",
            "style": "IPY_MODEL_cdea6ffd2b7a493687dde71ee7595af7",
            "value": "Filter: 100%"
          }
        },
        "75af803c409241e7b2e1ca55587e8b1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f335294b4974233b8a237ef617ec3a4",
            "max": 35,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_12389ee735e94d938144744d4e56c06a",
            "value": 35
          }
        },
        "7647b89f4c1d4ffd89cd29b6013766f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7bbab719f10d4e4ab6e67e420dabba4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9dd71500aeba4562b402acfc3882c3d9",
            "placeholder": "​",
            "style": "IPY_MODEL_e2fc75f63f0148f488ce9feec63d4777",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "7cee91c294cb4c2686fc113c139a1bc0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cf46113a5f244e9a1709fc170389e81": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7355a3104c47476f8b7e6b9f42e533f5",
            "placeholder": "​",
            "style": "IPY_MODEL_187d423fc4034cc0a10a1232b0c5c4e7",
            "value": " 1.16M/1.16M [00:00&lt;00:00, 5.30MB/s]"
          }
        },
        "7cf7afa6bd124911b5b4283e9be24cfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8190b6d1997f4e1c89773265f0392fba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_afb6fa57faf24bdd9341adbb072a7882",
            "placeholder": "​",
            "style": "IPY_MODEL_4e0bad8e4cea465d9b7edac93e7d0456",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "8318bc2f20d4492f94caf34ef2b7571d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_511c7b16b3614cd081adf400fd6551ca",
              "IPY_MODEL_faa51be94234432590e0bca4b8e3a1fb",
              "IPY_MODEL_114e4083103f4cb591176b004cc36121"
            ],
            "layout": "IPY_MODEL_d3abadfd0c9843a0b5aa682e6133c06b"
          }
        },
        "838a06168cf24449b180936fbfe1c232": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83bd570fb7934690acbebe9a862ce252": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8424b39c88b04360b06b4b2e6e3b9275": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "852fc02a5e914b4cb662b7aaf0ccfe81": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88c4a16bc95c4c4fb8472125d520f153": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8caf937884c947208f6a92e997552056": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e4e9fd5d5f14190934caf79cbe41dbd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90a13d3617f846be889454d0908fba16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90c48ab7058d427d989c1f80dcac1d36": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90cae1d2990d429d9c52e57149c042aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d795851fef7c41d2935179c58e3012d0",
            "placeholder": "​",
            "style": "IPY_MODEL_7647b89f4c1d4ffd89cd29b6013766f4",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "92f9f5a8debd4ae7a136815f05511acc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_73afcd7c0d784f439612cc45265d3d02",
              "IPY_MODEL_e80d08cac52d483288c82538e5908f91",
              "IPY_MODEL_d537bc58d0344ce0bac75a240ad250c3"
            ],
            "layout": "IPY_MODEL_5dae7f443e6a4bf58341e40bbb92473d"
          }
        },
        "93a5d7ca0d0b4bb8a8191d498ba28925": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "949f7cfe8ec547c38bbcb35510d705a6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95749baf28ba4e5abb676aba27df23fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db2ba0e7fa114b76815aaaabd7ec4d3c",
            "placeholder": "​",
            "style": "IPY_MODEL_8caf937884c947208f6a92e997552056",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "97126c5f837d4419a03d5d0dd802984d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9775dcfdc68142d58650b7c7ff3a9d7d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b05c3aabc094d5899a99601c901b57d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b3e3bc8606e4430af5aa58562c17732": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5d4978515e44c06aeb34ac40287b2c6",
            "placeholder": "​",
            "style": "IPY_MODEL_028af4ac533a4b9e8df1a68033188c55",
            "value": " 33.4M/33.4M [00:00&lt;00:00, 294MB/s]"
          }
        },
        "9dd71500aeba4562b402acfc3882c3d9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ff35b4a29f241fd8bbf1c166b0b5ca9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9775dcfdc68142d58650b7c7ff3a9d7d",
            "placeholder": "​",
            "style": "IPY_MODEL_9b05c3aabc094d5899a99601c901b57d",
            "value": " 2/2 [00:35&lt;00:00, 18.76s/it]"
          }
        },
        "a08d16bde9ef4556a7382fc41a2a7a5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a173419e200b4d158c3b4e54e9685cb5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab401dedd62b4d92a4ad8e954e2ea11d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad6742c624d14bddb6871b3d3eef2e61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01ac06dc070d45db8de78a8649b4ad0e",
            "max": 1156999,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_67f58c69ee7548c5853c80acbdefe497",
            "value": 1156999
          }
        },
        "afb6fa57faf24bdd9341adbb072a7882": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0b7d968c16b4060b8a4e508a02ac372": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59975630f23f40a3a46de2eea39f3cc7",
            "placeholder": "​",
            "style": "IPY_MODEL_7cf7afa6bd124911b5b4283e9be24cfd",
            "value": "added_tokens.json: 100%"
          }
        },
        "b105ae15a27340c8adc6c4faf4d0ead0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b61208412f79400caa0d04417c6d777c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_586692bc26b7478a98c17e405b21e038",
            "placeholder": "​",
            "style": "IPY_MODEL_97126c5f837d4419a03d5d0dd802984d",
            "value": "Map: 100%"
          }
        },
        "b7ee3c63932743e0817fd36ec9b8ef0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1de63a00515d46b3bd5a8137ad002ce6",
            "placeholder": "​",
            "style": "IPY_MODEL_f9ac8b8fac5645a0bc2df48862ea6228",
            "value": " 35.0/35.0 [00:00&lt;00:00, 2.25kB/s]"
          }
        },
        "b8cdbf57a4474a3184828fba840561ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b90f3c73749f4fd796348d59b3b6ade2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90cae1d2990d429d9c52e57149c042aa",
              "IPY_MODEL_e06253cb6854457b87d41a85f14da5dd",
              "IPY_MODEL_0f97e57823304ba983b9295120963b6e"
            ],
            "layout": "IPY_MODEL_471806eefc9845c4a1a5d3af312ef987"
          }
        },
        "bc1195417f7142bba8b40a67d93472fa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf55a0a85c494747890f0592efe14f8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55d1cc4c70944200a0f4fb8a52afa94f",
            "max": 215,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2f80ebb78f094b12ac800662ef02a49f",
            "value": 215
          }
        },
        "c18748d2ee0545b688dc12ef721b3ca5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8cdbf57a4474a3184828fba840561ab",
            "max": 33384568,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5112ac84b10a4446b5903c35e41dfcee",
            "value": 33384568
          }
        },
        "c28e314865044facb9b429d27c348c10": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3c9d06ea616458d8430d037d566bce9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a9f6313b39841c7918386895f37cde0",
            "max": 4689074,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_33718572ba5e48f5aa15f2cb19911c99",
            "value": 4689074
          }
        },
        "c5d831ca0aaf4678baf1cf4121ed81da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22d8a5705a6c4b6fa97bd6160f151083",
              "IPY_MODEL_1daacc7f22044f068e98acc66f25e28b",
              "IPY_MODEL_58c3d53cdeb9459395233b3dd1023db3"
            ],
            "layout": "IPY_MODEL_8424b39c88b04360b06b4b2e6e3b9275"
          }
        },
        "c7e0c8c806094c828b10aedd0b44ab46": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c99c03f2f0494ffa8e6593aaf66f4bbe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd0529ea983149bb9ee6cba5f33d8898": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cdea6ffd2b7a493687dde71ee7595af7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cecc9dd0c22f4549be6d904fd5bd1dd0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf15706bdd2343969d3e13c9209cd1b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfc62f2c66924a4fa9699c935b948ccf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7bbab719f10d4e4ab6e67e420dabba4b",
              "IPY_MODEL_ad6742c624d14bddb6871b3d3eef2e61",
              "IPY_MODEL_7cf46113a5f244e9a1709fc170389e81"
            ],
            "layout": "IPY_MODEL_037aa56ec52046b3b38c28cc435ef656"
          }
        },
        "d054fc604d9c4f0fb26de1fc62fb4f73": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1c3b562a12940528f20e227be549142": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d3abadfd0c9843a0b5aa682e6133c06b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d537bc58d0344ce0bac75a240ad250c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_277d08131e074dfebf6ddf1c718cb4b0",
            "placeholder": "​",
            "style": "IPY_MODEL_e492735f801e48d49a78853efd1bfeab",
            "value": " 596/596 [00:00&lt;00:00, 13067.32 examples/s]"
          }
        },
        "d58aa9846a3240afb7ba5111a3df02a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b61208412f79400caa0d04417c6d777c",
              "IPY_MODEL_faf27dcd78344eb39fda6a27ebd0b30c",
              "IPY_MODEL_2d2abb9d1bae49b0a033a9f4bb3384c4"
            ],
            "layout": "IPY_MODEL_cecc9dd0c22f4549be6d904fd5bd1dd0"
          }
        },
        "d795851fef7c41d2935179c58e3012d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d79eaa2d5eaa4bffbe5f0d848b40e780": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f48fd9c41de4eb8a7884aafd2f63748",
              "IPY_MODEL_6bc32dc487504058909d4bc788d622e0",
              "IPY_MODEL_e1af374ad1b0444fa295c5f1059c5d41"
            ],
            "layout": "IPY_MODEL_578ef4ea74a94ec383cb0418a3dfe08d"
          }
        },
        "d7f088351cc343fbaa78e7cacf8c52d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_368bd9a9ade0454d93860fc5028d934d",
              "IPY_MODEL_bf55a0a85c494747890f0592efe14f8c",
              "IPY_MODEL_5339b7456fc843019310edffb9b8da15"
            ],
            "layout": "IPY_MODEL_f2013afc5faa4ea19be50a6e3379f2a4"
          }
        },
        "db2ba0e7fa114b76815aaaabd7ec4d3c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e06253cb6854457b87d41a85f14da5dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a2a2949de1641f8bc8703e45088cbdc",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_285c8a4e7a1d450b8fa04ca55c6e2438",
            "value": 2
          }
        },
        "e0df088668624a0cbb0874b3c9ace1d5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1af374ad1b0444fa295c5f1059c5d41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc5c770565a44b8eb77247b9ae954ed6",
            "placeholder": "​",
            "style": "IPY_MODEL_4499de05ebff4fb98d1722f43c7e64fd",
            "value": " 596/596 [00:00&lt;00:00, 9293.41 examples/s]"
          }
        },
        "e297938f26704833b1d79d1f8156c778": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_95749baf28ba4e5abb676aba27df23fa",
              "IPY_MODEL_ef9f53c107a545c0b50e24b16361dab1",
              "IPY_MODEL_273afce93ff84324ac5a36ae7aa53bfb"
            ],
            "layout": "IPY_MODEL_55e26b0c25a84ca78b50fc728156db11"
          }
        },
        "e2adc90ee82749df9b52d10584f00712": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_51f939af465d455c988a11a5415523b0",
              "IPY_MODEL_c18748d2ee0545b688dc12ef721b3ca5",
              "IPY_MODEL_9b3e3bc8606e4430af5aa58562c17732"
            ],
            "layout": "IPY_MODEL_93a5d7ca0d0b4bb8a8191d498ba28925"
          }
        },
        "e2d7c11476cb47eb8d57d0295a4ec753": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2fc75f63f0148f488ce9feec63d4777": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e492735f801e48d49a78853efd1bfeab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5d4978515e44c06aeb34ac40287b2c6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5f8bf04f2ad4737b96686b811479433": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e80d08cac52d483288c82538e5908f91": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f00cf45ef977448e8b2ad13ccbe173fd",
            "max": 596,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_88c4a16bc95c4c4fb8472125d520f153",
            "value": 596
          }
        },
        "eaf3c23b242344ee85277bd3e9fafcee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_704a46879a3e42629a1d36860efd7e79",
            "placeholder": "​",
            "style": "IPY_MODEL_0da5230e1da3486bb0b8556e00499daa",
            "value": "tokenizer.model: 100%"
          }
        },
        "ebe59296438c4dd691f307d11ad5b7d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee8f3d949ea64716b46b2f861113c5b8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef9f53c107a545c0b50e24b16361dab1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6113e27f1309421f84206cafbe358fb5",
            "max": 662,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d1c3b562a12940528f20e227be549142",
            "value": 662
          }
        },
        "f00cf45ef977448e8b2ad13ccbe173fd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2013afc5faa4ea19be50a6e3379f2a4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9ac8b8fac5645a0bc2df48862ea6228": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "faa51be94234432590e0bca4b8e3a1fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_549a07be71654c59ba8f6ae295088f6a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_41accdd5411c4a6eae76697fec3125a1",
            "value": 1
          }
        },
        "faf27dcd78344eb39fda6a27ebd0b30c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7e0c8c806094c828b10aedd0b44ab46",
            "max": 596,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0cc85a59f30140128fc04c065e0c526c",
            "value": 596
          }
        },
        "fc5c770565a44b8eb77247b9ae954ed6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fed6f52532804b73b7fd967ac541b181": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b0aece87c934f1fb014f26307ba5b07",
              "IPY_MODEL_261508f501d5412ea7de5a41ee90b80a",
              "IPY_MODEL_04e9be96f5a746ce98fdab6511102b82"
            ],
            "layout": "IPY_MODEL_90c48ab7058d427d989c1f80dcac1d36"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
