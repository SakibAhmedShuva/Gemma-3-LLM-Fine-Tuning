{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7177892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, load_from_disk # Added load_from_disk\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "import random\n",
    "import gc # For memory cleanup\n",
    "import traceback # For error details\n",
    "\n",
    "# --- Configuration ---\n",
    "# Input Data Paths\n",
    "skill_data_path = '/content/data/matey_skill_level_mapping.json'\n",
    "rules_data_path = '/content/data/matey_behavior_rules.json'\n",
    "tools_data_path = '/content/data/TotalTools.json'\n",
    "\n",
    "# Output Paths for Prepared Data\n",
    "output_data_dir = \"./prepared_matey_data\" # Main directory for prepared data\n",
    "skills_formatted_path = os.path.join(output_data_dir, \"skills_formatted.jsonl\")\n",
    "rules_formatted_path = os.path.join(output_data_dir, \"rules_formatted.jsonl\")\n",
    "tools_formatted_path = os.path.join(output_data_dir, \"tools_formatted.jsonl\")\n",
    "combined_formatted_path = os.path.join(output_data_dir, \"combined_formatted.jsonl\")\n",
    "hf_dataset_path = os.path.join(output_data_dir, \"hf_matey_dataset_split\") # Path to save DatasetDict\n",
    "\n",
    "# Training Configuration\n",
    "model_id = \"google/gemma-3-4b-it\" #\"google/gemma-2b\"\n",
    "training_output_dir = \"./gemma-matey-finetuned-from-saved-data\" # Trainer checkpoints\n",
    "final_adapter_path = \"./gemma-matey-lora-adapter-from-saved-data\" # Final adapter\n",
    "\n",
    "# Ensure output data directory exists\n",
    "os.makedirs(output_data_dir, exist_ok=True)\n",
    "\n",
    "# Ignore specific warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", message=\"`tokenizer` is deprecated and will be removed in version 5.0.0\")\n",
    "os.environ['WANDB_DISABLED'] = 'true' # Disable wandb logging\n",
    "\n",
    "# --- Helper function to load JSON data ---\n",
    "def load_json_data(filepath, sheet_key=None):\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        if sheet_key:\n",
    "            if sheet_key in data and isinstance(data[sheet_key], list):\n",
    "                data = data[sheet_key]\n",
    "            else: return []\n",
    "        if not isinstance(data, list): return []\n",
    "        print(f\"Loaded {len(data)} entries from {filepath}\" + (f\" (key '{sheet_key}')\" if sheet_key else \"\"))\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filepath}: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Formatting Functions ---\n",
    "def format_skill_or_rule(example, data_type):\n",
    "    user_prompt = \"\"\n",
    "    model_response = \"\"\n",
    "    if data_type == 'skill' and 'skill_level' in example and 'advice' in example:\n",
    "        user_prompt = f\"What's your advice for a {example['skill_level'].lower()} DIYer like me?\"\n",
    "        model_response = example['advice']\n",
    "    elif data_type == 'rule' and 'situation' in example and 'instruction' in example:\n",
    "        user_prompt = f\"How should I handle this situation: {example['situation']}?\"\n",
    "        model_response = example['instruction']\n",
    "    else: return None\n",
    "    return f\"<start_of_turn>user\\n{user_prompt}<end_of_turn>\\n<start_of_turn>model\\n{model_response}<end_of_turn>\"\n",
    "\n",
    "def format_tool_data(tool_entry):\n",
    "    try:\n",
    "        tool_name = tool_entry.get(\"Tool name\", \"this tool\")\n",
    "        user_prompt = f\"Tell me about the {tool_name}.\"\n",
    "        brand = tool_entry.get(\"Brand\", \"N/A\")\n",
    "        tool_type = tool_entry.get(\"Tool Type\", \"tool\")\n",
    "        price = tool_entry.get(\"Price on Total Tools\", \"N/A\")\n",
    "        corded_status = tool_entry.get(\"Corded / Cordless\", \"\")\n",
    "        response_parts = [f\"Righto, the {tool_name} is a {corded_status} {tool_type} from {brand}.\"]\n",
    "        if price != \"N/A\":\n",
    "            try: # Handle potential price formatting issues\n",
    "               response_parts.append(f\"Looks like it goes for about ${float(price):.2f} at Total Tools.\")\n",
    "            except (ValueError, TypeError):\n",
    "                response_parts.append(f\"Looks like it goes for about {price} at Total Tools.\") # Use raw value if conversion fails\n",
    "\n",
    "        power = tool_entry.get(\"Rated Power (w)\") or tool_entry.get(\"Power Rating (w)\")\n",
    "        voltage = tool_entry.get(\"Voltage\")\n",
    "        if power: response_parts.append(f\"It's rated at {power}W.\")\n",
    "        if voltage: response_parts.append(f\"Runs on {voltage}.\")\n",
    "        model_response = \" \".join(response_parts)\n",
    "        return f\"<start_of_turn>user\\n{user_prompt}<end_of_turn>\\n<start_of_turn>model\\n{model_response}<end_of_turn>\"\n",
    "    except Exception: return None\n",
    "\n",
    "# --- Helper function to save formatted data to JSONL ---\n",
    "def save_formatted_jsonl(data_list, filepath):\n",
    "    try:\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            for text_string in data_list:\n",
    "                # Each line is a JSON object with a \"text\" key\n",
    "                json.dump({\"text\": text_string}, f)\n",
    "                f.write('\\n')\n",
    "        print(f\"Saved {len(data_list)} formatted entries to {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to {filepath}: {e}\")\n",
    "\n",
    "# ======================================================================\n",
    "#                            PHASE 1: DATA PREPARATION\n",
    "# ======================================================================\n",
    "print(\"\\n\" + \"=\"*20 + \" PHASE 1: Preparing and Saving Data \" + \"=\"*20)\n",
    "\n",
    "# 1. Load original data\n",
    "skill_data_raw = load_json_data(skill_data_path)\n",
    "rules_data_raw = load_json_data(rules_data_path)\n",
    "tool_data_raw = load_json_data(tools_data_path, sheet_key=\"Sheet1\")\n",
    "\n",
    "# 2. Format data separately\n",
    "formatted_skills = [fmt for entry in skill_data_raw if (fmt := format_skill_or_rule(entry, 'skill')) is not None]\n",
    "formatted_rules = [fmt for entry in rules_data_raw if (fmt := format_skill_or_rule(entry, 'rule')) is not None]\n",
    "formatted_tools = [fmt for entry in tool_data_raw if (fmt := format_tool_data(entry)) is not None]\n",
    "\n",
    "print(f\"Formatted {len(formatted_skills)} skill entries.\")\n",
    "print(f\"Formatted {len(formatted_rules)} rule entries.\")\n",
    "print(f\"Formatted {len(formatted_tools)} tool entries.\")\n",
    "\n",
    "# 3. Save formatted data individually\n",
    "if formatted_skills: save_formatted_jsonl(formatted_skills, skills_formatted_path)\n",
    "if formatted_rules: save_formatted_jsonl(formatted_rules, rules_formatted_path)\n",
    "if formatted_tools: save_formatted_jsonl(formatted_tools, tools_formatted_path)\n",
    "\n",
    "# 4. Combine, Shuffle, and Save Combined Data\n",
    "formatted_combined = formatted_skills + formatted_rules + formatted_tools\n",
    "if not formatted_combined:\n",
    "    raise ValueError(\"No data could be formatted from any source file.\")\n",
    "\n",
    "print(f\"\\nTotal combined formatted examples: {len(formatted_combined)}\")\n",
    "random.shuffle(formatted_combined)\n",
    "print(\"Shuffled the combined dataset.\")\n",
    "save_formatted_jsonl(formatted_combined, combined_formatted_path)\n",
    "\n",
    "# 5. Create HF Dataset, Split, and Save to Disk\n",
    "print(\"\\nCreating and splitting Hugging Face Dataset...\")\n",
    "# Check if combined data exists before creating dataset\n",
    "if os.path.exists(combined_formatted_path):\n",
    "    # Load directly from the JSONL file if needed, or use the in-memory list\n",
    "    # Using the in-memory list 'formatted_combined' here\n",
    "    combined_hf_dataset = Dataset.from_dict({\"text\": formatted_combined})\n",
    "\n",
    "    # Split into train/test (e.g., 90% train, 10% test)\n",
    "    split_dataset_dict = combined_hf_dataset.train_test_split(test_size=0.1, seed=42) # Use seed for reproducibility\n",
    "    print(\"Dataset split:\")\n",
    "    print(split_dataset_dict)\n",
    "\n",
    "    # Save the DatasetDict (contains train/test splits) to disk\n",
    "    try:\n",
    "        split_dataset_dict.save_to_disk(hf_dataset_path)\n",
    "        print(f\"Saved split DatasetDict to disk at: {hf_dataset_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving HF DatasetDict to disk: {e}\")\n",
    "        traceback.print_exc() # Print full traceback\n",
    "else:\n",
    "    print(f\"ERROR: Combined formatted data file not found at {combined_formatted_path}. Cannot create HF dataset.\")\n",
    "    exit()\n",
    "\n",
    "print(\"\\n\" + \"=\"*20 + \" PHASE 1: Data Preparation Complete \" + \"=\"*20)\n",
    "print(f\"Prepared data saved in: {output_data_dir}\")\n",
    "print(f\"Split train/test dataset saved in HF format at: {hf_dataset_path}\")\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "#                       PHASE 2: TRAINING (Manual Load)\n",
    "# ======================================================================\n",
    "# This part would typically be run separately or after confirming Phase 1 is done.\n",
    "\n",
    "print(\"\\n\" + \"=\"*20 + \" PHASE 2: Loading Prepared Data and Training \" + \"=\"*20)\n",
    "\n",
    "# 1. Load the prepared dataset from disk\n",
    "print(f\"\\nLoading prepared dataset from: {hf_dataset_path}\")\n",
    "if not os.path.exists(hf_dataset_path):\n",
    "     print(f\"ERROR: Prepared dataset directory not found at {hf_dataset_path}. Make sure Phase 1 ran successfully and saved the data.\")\n",
    "     exit() # Stop if data isn't ready\n",
    "\n",
    "try:\n",
    "    loaded_dataset_dict = load_from_disk(hf_dataset_path)\n",
    "    print(\"Successfully loaded split dataset from disk:\")\n",
    "    print(loaded_dataset_dict)\n",
    "    train_dataset = loaded_dataset_dict['train']\n",
    "    eval_dataset = loaded_dataset_dict['test'] # Use this for evaluation\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to load dataset from disk: {e}\")\n",
    "    traceback.print_exc()\n",
    "    exit()\n",
    "\n",
    "\n",
    "# 2. Load Model and Tokenizer\n",
    "print(\"\\n--- Loading Model and Tokenizer ---\")\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    if tokenizer.eos_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(f\"Set pad_token to eos_token: {tokenizer.pad_token}\")\n",
    "    else:\n",
    "        pad_token_str = '[PAD]'\n",
    "        tokenizer.add_special_tokens({'pad_token': pad_token_str})\n",
    "        print(f\"Added and set pad_token to: {pad_token_str}\")\n",
    "        # Important: Resize model embeddings if a new token was added\n",
    "        # model.resize_token_embeddings(len(tokenizer)) # Do this AFTER loading model\n",
    "\n",
    "# Load model WITH the recommended attention implementation\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\" # <--- ADDED THIS LINE based on warning\n",
    ")\n",
    "print(\"Base model loaded with attn_implementation='eager'.\")\n",
    "\n",
    "# Resize embeddings if pad token was added *after* loading the model\n",
    "if tokenizer.pad_token == '[PAD]': # Check if we added it\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(\"Resized model token embeddings for new pad token.\")\n",
    "\n",
    "\n",
    "# 3. Setup LoRA\n",
    "print(\"\\n--- Setting up LoRA ---\")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "lora_config = LoraConfig(\n",
    "    r=16, lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"LoRA adapter applied.\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# 4. Tokenize the Loaded Datasets\n",
    "print(\"\\n--- Tokenizing Loaded Datasets ---\")\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the 'text' column from the loaded dataset\n",
    "    # Ensure padding and truncation are handled. Max length might need adjustment.\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512, padding=False) # Let DataCollator handle padding\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "print(\"Train and Eval datasets tokenized.\")\n",
    "print(f\"Tokenized Train Dataset: {tokenized_train_dataset}\")\n",
    "print(f\"Tokenized Eval Dataset: {tokenized_eval_dataset}\")\n",
    "\n",
    "\n",
    "# 5. Configure and Run Training\n",
    "print(\"\\n--- Configuring Training ---\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=training_output_dir,\n",
    "    per_device_train_batch_size=10, # Adjusted batch size (start small if needed)\n",
    "    gradient_accumulation_steps=8, # Effective batch size = 32\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=10, # Adjusted epochs (start with fewer)\n",
    "    logging_steps=20,    # Log more frequently with smaller steps\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\", # Evaluate at the end of each epoch\n",
    "    load_best_model_at_end=True, # Load the best model based on eval loss\n",
    "    metric_for_best_model=\"eval_loss\", # Explicitly state the metric\n",
    "    greater_is_better=False, # Lower eval_loss is better\n",
    "    fp16=False, # Use bf16 as specified below\n",
    "    bf16=True,  # Use bfloat16 for better performance on compatible hardware\n",
    "    gradient_checkpointing=True, # Saves memory\n",
    "    report_to=\"none\", # Keep wandb disabled\n",
    "    # Added arguments:\n",
    "    optim=\"paged_adamw_8bit\", # Optimizer suitable for QLoRA\n",
    "    lr_scheduler_type=\"cosine\", # Learning rate scheduler\n",
    "    warmup_ratio=0.03, # Warmup steps as a ratio of total steps\n",
    ")\n",
    "# Use DataCollatorForLanguageModeling for Causal LM\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset, # Pass the evaluation dataset\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"\\n--- Starting LoRA Fine-Tuning ---\")\n",
    "try:\n",
    "    train_result = trainer.train() # Trainer will handle resuming from checkpoints\n",
    "    print(\"Training finished.\")\n",
    "    trainer.save_state()\n",
    "    print(\"Trainer state saved.\")\n",
    "\n",
    "    # Log training metrics\n",
    "    metrics = train_result.metrics\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "\n",
    "    # Log evaluation metrics\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    trainer.log_metrics(\"eval\", eval_metrics)\n",
    "    trainer.save_metrics(\"eval\", eval_metrics)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during training: {e}\")\n",
    "    traceback.print_exc()\n",
    "    # Optionally try saving adapter even if training failed partially\n",
    "    # try:\n",
    "    #     print(\"Attempting to save adapter despite training error...\")\n",
    "    #     model.save_pretrained(final_adapter_path + \"_partial\")\n",
    "    #     tokenizer.save_pretrained(final_adapter_path + \"_partial\")\n",
    "    #     print(\"Partially trained adapter saved.\")\n",
    "    # except Exception as e_save:\n",
    "    #     print(f\"Could not save partial adapter: {e_save}\")\n",
    "    exit() # Stop script if training fails\n",
    "\n",
    "# 6. Save Final Adapter (Best Model based on Eval)\n",
    "print(f\"\\n--- Saving Best LoRA Adapter to: {final_adapter_path} ---\")\n",
    "# Trainer already loaded the best model if load_best_model_at_end=True\n",
    "# We just need to save it using the PEFT method\n",
    "try:\n",
    "    model.save_pretrained(final_adapter_path) # Save the best PEFT adapter\n",
    "    tokenizer.save_pretrained(final_adapter_path)\n",
    "    print(\"Best adapter and tokenizer saved.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving final adapter/tokenizer: {e}\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "\n",
    "# 7. Optional: Inference Test\n",
    "print(\"\\n--- Running Inference Test with Saved Adapter ---\")\n",
    "# Clear memory before loading for inference\n",
    "try:\n",
    "    del model\n",
    "    del trainer\n",
    "    if 'base_model_inf' in locals(): del base_model_inf\n",
    "    if 'inference_model' in locals(): del inference_model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Cleaned up memory.\")\n",
    "    print(\"Reloading base model and loading best adapter for inference...\")\n",
    "\n",
    "    # Reload tokenizer just in case\n",
    "    tokenizer_inf = AutoTokenizer.from_pretrained(final_adapter_path)\n",
    "\n",
    "    # Load base model again with quantization and recommended attn impl\n",
    "    base_model_inf = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16, # Match training dtype if possible\n",
    "        attn_implementation=\"eager\" # <--- ADDED THIS LINE for consistency\n",
    "    )\n",
    "\n",
    "    # Load the saved LoRA adapter\n",
    "    inference_model = PeftModel.from_pretrained(base_model_inf, final_adapter_path)\n",
    "    inference_model = inference_model.eval() # Set to evaluation mode\n",
    "    print(\"Inference model ready.\")\n",
    "\n",
    "\n",
    "    print(\"\\nGenerating response (Tool Query)...\")\n",
    "    prompt_tool = \"<start_of_turn>user\\nTell me about the BOSCH 750W 125MM ANGLE GRINDER 0601394042.<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    inputs_tool = tokenizer_inf(prompt_tool, return_tensors=\"pt\").to(inference_model.device)\n",
    "\n",
    "    # Generation parameters\n",
    "    generation_config = {\n",
    "        \"max_new_tokens\": 150,\n",
    "        \"temperature\": 0.7, # Slightly increased temperature for more varied output\n",
    "        \"top_p\": 0.9,       # Nucleus sampling\n",
    "        \"do_sample\": True,\n",
    "        \"pad_token_id\": tokenizer_inf.eos_token_id # Use EOS token for padding during generation\n",
    "    }\n",
    "\n",
    "    print(f\"Using generation config: {generation_config}\")\n",
    "    with torch.no_grad():\n",
    "        outputs_tool = inference_model.generate(\n",
    "            **inputs_tool,\n",
    "            **generation_config\n",
    "        )\n",
    "\n",
    "    # Decode only the generated part, excluding the prompt\n",
    "    # generated_ids = outputs_tool[0][inputs_tool.input_ids.shape[1]:]\n",
    "    # generated_text_tool = tokenizer_inf.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Decode the full output sequence including the prompt for context\n",
    "    full_generated_text_tool = tokenizer_inf.decode(outputs_tool[0], skip_special_tokens=False)\n",
    "\n",
    "\n",
    "    print(\"\\n--- Generated Tool Response (Full Sequence) ---\")\n",
    "    print(full_generated_text_tool)\n",
    "    # print(\"\\n--- Generated Tool Response (Generated Part Only) ---\")\n",
    "    # print(generated_text_tool)\n",
    "\n",
    "\n",
    "    print(\"\\nGenerating response (Skill Query)...\")\n",
    "    prompt_skill = \"<start_of_turn>user\\nWhat's your advice for a beginner DIYer like me?<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    inputs_skill = tokenizer_inf(prompt_skill, return_tensors=\"pt\").to(inference_model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs_skill = inference_model.generate(\n",
    "             **inputs_skill,\n",
    "            **generation_config\n",
    "        )\n",
    "    full_generated_text_skill = tokenizer_inf.decode(outputs_skill[0], skip_special_tokens=False)\n",
    "    print(\"\\n--- Generated Skill Response (Full Sequence) ---\")\n",
    "    print(full_generated_text_skill)\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR during inference: Adapter not found at {final_adapter_path}. Was training successful?\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during inference test: {e}\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n--- Script Finished ---\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
